<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>k8s核心原理 on Realjf&#39;s blog</title>
    <link>https://realjf.io/tags/k8s%E6%A0%B8%E5%BF%83%E5%8E%9F%E7%90%86/</link>
    <description>Recent content in k8s核心原理 on Realjf&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 19 Mar 2019 14:30:13 +0800</lastBuildDate><atom:link href="https://realjf.io/tags/k8s%E6%A0%B8%E5%BF%83%E5%8E%9F%E7%90%86/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>kubernetes 核心原理之 网络模型</title>
      <link>https://realjf.io/posts/k8s-core-principle-network-model/</link>
      <pubDate>Tue, 19 Mar 2019 14:30:13 +0800</pubDate>
      
      <guid>https://realjf.io/posts/k8s-core-principle-network-model/</guid>
      <description>主要解决以下问题：
 容器与容器之间的直接通信 pod与pod之间的通信 pod到service之间的通信 集群外部与集群内部组件之间的通信  容器与容器之间的通信 同一个Pod内的容器共享同一个linux协议栈，可以用localhost地址访问彼此的端口 kubernetes利用docker的网桥与容器内的映射eth0设备进行通信
pod之间的通信 每个pod都拥有一个真实的全局ip地址
 同一个node内的不同pod之间 可以直接采用对方的pod的ip地址通信（因为他们都在同一个docker0网桥上，属于同一地址段） 不同node上的pod之间  </description>
    </item>
    
    <item>
      <title>kubernetes 核心原理之 集群安全机制</title>
      <link>https://realjf.io/posts/k8s-core-principle-security/</link>
      <pubDate>Tue, 19 Mar 2019 14:29:44 +0800</pubDate>
      
      <guid>https://realjf.io/posts/k8s-core-principle-security/</guid>
      <description>安全性考虑目标  保证容器与其所在的宿主机的隔离 限制容器给基础设施及其他容器带来消极影响的能力 最小权限原则 明确组件间边界的划分 划分普通用户和管理员的角色 在必要时允许将管理员权限赋给普通用户 允许拥有secret数据（Keys、Certs、Passwords）的应用在集群中运行  1. API Server认证管理（Authentication） 集群安全的关键就在于如何识别并认证客户端身份，以及随后访问权限的授权这两个关键问题
k8s提供3种级别的客户端身份认证方式：
 最严格的https证书认证：基于ca根证书签名的双向数字证书认证 http token认证：通过一个token来识别合法用户   http token用一个很长的特殊编码方式并且难以被模仿的字符串——token来表明客户端身份，每个token对应一个用户名，存储在api server能访问的一个文件中，当客户端发起api调用请求时，需要在http header里放入token，这样一来，api server就能识别合法用户和非法用户了。
  http base认证：通过用户名+密码的方式   http base是指把“用户名+冒号+密码”用base64算法进行编码后的字符串放在http request中的header authorization域里发送到服务端，服务端接受后进行解码，获取用户名及密码，然后进行用户身份鉴权过程
 2. API Server授权管理（Authorization） 通过授权策略来决定一个api调用是否合法。对合法用户进行授权并且随后在用户访问时进行鉴权，是权限与安全系统的重要一环。
目前支持的授权策略：
 AlwaysDeny：表示拒绝所有的请求，一般用于测试 AlwaysAllow：允许接受所有请求，如果集群不需要授权，则可以采用这个策略，这也是默认配置 ABAC：基于属性的访问控制，表示使用用户配置的授权规则对用户请求进行匹配和控制。 Webhook：通过调用外部rest服务对用户进行授权 RBAC：基于角色的访问控制  ABAC授权模式 Webhook授权模式 RBAC授权模式详解 基于角色的访问控制：
 对集群中的资源和非资源权限均有完整的覆盖 整个RBAC完全由几个api对象完成，同其他api对象一样，可以用kubectl或api进行操作 可以在运行时进行调整，无需重新启动api server   要使用RBAC授权模式，需要在api server的启动参数中加上 &amp;ndash;authorization-mode=RBAC
 3. Admission Control（准入控制） </description>
    </item>
    
    <item>
      <title>kubernetes 核心原理之 Kube-Proxy</title>
      <link>https://realjf.io/posts/k8s-core-principle-kube-proxy/</link>
      <pubDate>Tue, 19 Mar 2019 14:26:43 +0800</pubDate>
      
      <guid>https://realjf.io/posts/k8s-core-principle-kube-proxy/</guid>
      <description>service是一个抽象的概念，类似一个反向代理，将请求转发到后端的pod上。真正实现service作用的是kube-proxy服务进程。
在每个node上都会运行一个kube-proxy的服务进程，这个进程可以看做service的透明代理兼负载均衡器，其核心功能是将到某个service的访问请求转发到后端的多个pod实例上。
kube-proxy会在本地node上简历一个socketserver来负责接收请求，然后均匀发送到后端某个pod端口上，这个过程默认采用round robin负载均衡算法。
k8s也提供了通过修改service的service.spec.sessionAffinity参数的值来实现会话保持特性的定向转发，如果设置的值为“clientIp”，则将来自同一个clientip的请求都转发到同一个后端pod上。
 service的clusterIP与nodeport等概念是kube-proxy服务通过iptables的NAT转换实现的，kube-proxy在运行过程中动态创建与service相关的iptables规则
 访问service的请求，不论是用cluster ip+target port的方式，还是用节点机ip+node port的方式，都被节点机的iptables规则重定向到kube-proxy监听的service服务代理端口。
 kube-proxy的负载均衡器只支持round robin算法。同时在此基础上还支持session保持。
 kube-proxy内部创建了一个负载均衡器——loadbalancer，loadbalancer上保存了service到对应的后端endpoint列表的动态转发路由表，而具体的路由选择取决于round robin负载均衡算法及service的session会话保持（SessionAffinity）这两个特性
kube-porxy针对变化的service列表，处理流程  如果service没有设置集群ip（ClusterIP），则不做处理，否则，获取该service的所有端口定义列表（spec.ports域） 逐个读取服务端口定义列表中的端口信息，根据端口名称，service名称和namespace判断本地是否已经存在对应的服务代理对象，如不存在则新建，如存在且service端口被修改过，则先删除iptables中和srevice端口相关的规则，关闭服务代理对象，然后走新建流程。 更新负载均衡器组件中对应service的转发地址列表，对于新建的service，确定转发时的会话保持策略。 对于已经删除的service则进行清理   针对Endpoint的变化，kube-proxy会自动更新负载均衡器中对应service的转发地址列表。
 针对iptables所做的一些细节操作  KUBE-PORTALS-CONTAINER：从容器中通过service cluster ip和端口号访问service的请求。（容器） KUBE-PORTALS-HOST：从主机中通过service cluster ip和端口号访问service的请求（主机） KUBE-NODEPORT-CONTAINER：从容器中通过service的nodeport端口号访问service的请求。（容器） KUBE-NODEPORT-HOST：从主机中通过service的nodeport端口号访问service请求（主机）  此外，kube-proxy在iptables中为每个service创建由cluster ip+service端口号到kube-proxy所在主机ip+service代理服务所监听的端口的转发规则。
service类型为NodePort kube-proxy在iptables中除了添加上面提及的规则，还会为每个service创建由nodeport端口到kube-proxy所在主机ip+service代理服务所监听的端口的转发规则。</description>
    </item>
    
    <item>
      <title>kubernetes 核心原理之 Kubelet</title>
      <link>https://realjf.io/posts/k8s-core-principle-kubelet/</link>
      <pubDate>Tue, 19 Mar 2019 14:26:28 +0800</pubDate>
      
      <guid>https://realjf.io/posts/k8s-core-principle-kubelet/</guid>
      <description>在每个Node节点上都会启动一个kubelet服务进程，该进程负责处理master节点下发到本节点的任务，管理Pod和pod中的容器。
每个kubelet进程会在api server上注册节点自身信息，定期向master节点汇报节点资源的使用情况，并通过cAdvisor监控容器和节点资源。
节点管理 节点通过设置kubelet的启动参数“&amp;ndash;register-node”，来决定是否向api server注册自己，如果该参数为true，则会向api server注册自己。
其他参数包括：
 &amp;ndash;api-servers：api server的位置 &amp;ndash;kubeconfig：kubeconfig文件，用于访问api server的安全配置文件 &amp;ndash;cloud-provider：云服务商地址，仅用于公有云环境  通过kubelet的启动参数“&amp;ndash;node-status-update-frequency”设置kubelet每隔多长时间想api server报告节点状态，默认是10s。
Pod管理 kubelet通过以下几种方式获取自身node上所要运行的pod清单：
 文件：同过启动参数“&amp;ndash;config”指定的配置文件目录下的文件（默认/etc/kubernetes/manifests/） http断电：通过“&amp;ndash;manifest-url”参数设置 api server：通过api server监听etcd目录，同步pod列表  kubelet去读监听到的信息，如果是创建和修改pod任务，则  为该pod创建一个数据目录 从api server读取该pod清单 为该pod挂载外部卷 下载pod用到的secret 检查已经运行在节点中的pod，如果该pod没有容器或pause容器（kubernetes/pause镜像创建的容器）没有启动，则先停止pod里所有容器的进程。如果在pod中有需要删除的容器，则删除这些容器。 用&amp;quot;kubernetes/pause&amp;quot;镜像为每个pod创建一个容器，该pause容器用于接管pod中所有其他容器的网络。每创建一个新的pod，kubelet都会先创建一个pause容器，然后创建其他容器。 为pod中的每个容器做如下处理：   为容器计算一个hash值，然后用容器的名字去查询对应docker容器的hash值。若找到容器，且两者的hash值不同，则停止docker中容器的进程，并停止与之关联的pause容器的进程，若两者相同，则不做任何处理。 如果容器被终止了，且容器没有指定的restartPolicy（重启策略），则不做任何处理。 调用docker client下载容器镜像，调用docker client运行容器。  容器健康检查 检查容器健康状态的两种探针
 LivenessProbe探针：判断容器是否健康，如果不健康，则删除Pod，根据其重启策略做相应处理。 ReadinessProbe探针：判断容器是否完成启动，且准备接受请求。如果失败，pod的状态将被修改，Endpoint Controller将从Service的Endpoint中删除包含该容器所在pod的ip地址的endpoint条目。  LivenessProbe实现方式  ExecAction：在容器内部执行一个命令，如果该命令的退出状态码为0，则表明容器健康 TCPSocketAction：通过容器的ip地址和端口号执行TCP检查，如果端口能被访问，则表明容器健康 HTTPGetAction：通过容器的ip地址和端口号即路径调用http get方法，如果响应的状态码大于等于200且小于400，则认为容器状态健康  LivenessProbe探针包含在pod定义的spec.containers.{某个容器}中
# 容器命令检查 livenessProbe: exec: command: - cat - /tmp/health initialDelaySeconds: 15 timeoutSeconds: 1 # http检查 livenessProbe: httpGet: path: /healthz port: 8080 initialDelaySeconds: 15 timeoutSeconds: 1 cAdvisor资源监控 监控级别包括：容器、pod、service和整个集群</description>
    </item>
    
    <item>
      <title>kubernetes 核心原理之 Sheduler</title>
      <link>https://realjf.io/posts/k8s-core-principle-sheduler/</link>
      <pubDate>Tue, 19 Mar 2019 14:26:15 +0800</pubDate>
      
      <guid>https://realjf.io/posts/k8s-core-principle-sheduler/</guid>
      <description>作用是将待调度的pod按照特定的调度算法和调度策略绑定到集群中的某个合适的Node上，并将绑定信息写入etcd中。
目标节点上的kubelet通过api server监听到schduler产生的pod绑定事件，然后获取对应的pod清单，下载image镜像，并启动容器。
Scheduler默认调度流程分为以下两步  预调度过程，即遍历所有目标node，筛选出符合要求的候选节点 确定最优节点，在上一步基础上，采用优选策略计算出每个候选节点的积分，积分高者胜出。  Scheduler调度流程是通过插件方式加载的“调度算法提供者”（AlgorithmProvider）具体实现的。一个AlgorithmProvider其实是一组预选策略与一组优先选择策略的结构体。
Scheduler中可选的预选策略  NoDiskConflict PodFitsResources PodSelectorMatches PodFitsHost CheckNodeLabelPresence CheckServiceAffinity PodFitsPorts  Scheduler优选策略  LeastRequestedPriority（资源消耗最小） CalculateNodeLabelPriority BalancedResourceAllocation（各项资源使用率最均衡的节点）  每个节点通过优选策略算出一个得分，最终选出分值最大的节点作为优选的结果。</description>
    </item>
    
    <item>
      <title>kubernetes 核心原理之 Controller Manager</title>
      <link>https://realjf.io/posts/k8s-core-principle-controller-manager/</link>
      <pubDate>Tue, 19 Mar 2019 14:26:00 +0800</pubDate>
      
      <guid>https://realjf.io/posts/k8s-core-principle-controller-manager/</guid>
      <description>controller manager作为集群内部的管理控制中心，负责集群内的Node、pod副本、服务端（Endpoint）、命名空间（Namespace）、服务账号（ServiceAccount）、资源定额（ResourceQuota）等的管理。
controller manager组件  replication controller node controller resourceQuota controller namespace controller serviceAccount controller token controller service controller endpoint controller  1. Replication Controller（副本控制器） 核心作用是确保在任何时候集群中一个RC所关联的pod副本数量保持预设值。
 只有当pod的重启策略是always时（RestartPolicy=Always），Replication Controller才会管理该Pod的操作（创建、销毁、重启等）。
 RC中的Pod模板就像一个模具，一旦pod被创建完毕，它们之间就没有关系了。
此外，可以通过修改Pod的标签来实现脱离RC的管控。该方法可以用于将pod从集群中迁移、数据修复等调试。
Replication Controller职责  确保当前集群中有且仅有N个pod实例，N是RC中定义的pod副本数量 通过调整RC的spec.replicas属性值来实现系统扩容或者缩容 通过改变RC中的pod模板（主要是镜像版本）来实现系统的滚动升级  Replication Controller典型使用场景  重新调度（Rescheduling） 弹性伸缩（Scaling） 滚动更新（Rolling Updates）  2. Node Controller Node Controller通过API Server实时获取Node的相关信息：节点健康状况、节点资源、节点名称、节点地址信息、操作系统版本、Docker版本、kubelet版本等。
node controller节点信息更新机制 比较节点信息和node controller的nodeStatusMap中保存的节点信息
 如果没有收到kubelet发送的节点信息、第一次收到节点kubelet发送的节点信息，或处理过程中节点状态变成非健康状态，则在nodeStatusMap中保存改节点的状态信息，并用node controller所在节点的系统时间作为探测时间和节点状态变化时间。 如果指定时间内收到新的节点信息，且节点状态发生变化，则在nodeStatusMap保存改节点的状态信息，并用node controller所在节点的系统时间作为探测时间，用上次节点信息中的节点状态变化时间作为该节点的状态变化时间 如果某一段时间内没有收到该节点状态信息，则设置节点状态为未知，并通过api server保存节点状态  3. ResourceQuota Controller（资源配额管理） 资源配额管理确保了指定的资源对象在任何时候都不会超量占用系统物理资源，避免由于某些业务进程的设计或实现的缺陷导致整个系统运行紊乱甚至意外宕机
k8s支持一下三个层次的资源配额管理  容器级别，可以对cpu和memory进行限制 pod级别，可以对一个pod内所有容器的可用资源进行限制 namespace级别，为namespace（多租户）级别的资源限制，包括：pod数量、replication controller数量，service数量，resourceQuota数量，secret数量，可持有的pv数量  配额管理通过admission control来控制，admission control当前提供两种方式的配额约束，分别是limitRanger与resourceQuota。</description>
    </item>
    
    <item>
      <title>kubernetes 核心原理之 Apiserver</title>
      <link>https://realjf.io/posts/k8s-core-principle-apiserver/</link>
      <pubDate>Tue, 19 Mar 2019 14:25:43 +0800</pubDate>
      
      <guid>https://realjf.io/posts/k8s-core-principle-apiserver/</guid>
      <description>API Server的核心功能提供了kubernetes各类资源对象（如pod、rc、service等）的增删改查以及watch等http rest接口，是集群内各个功能模块之间数据交互和通信的中心枢纽，是整个系统的数据总线和数据中心。
 是集群管理的api入口 是资源配额控制的入口 提供了完备的集群安全机制  </description>
    </item>
    
  </channel>
</rss>
