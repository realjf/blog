<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Realjf&#39;s blog</title>
    <link>https://realjf.io/</link>
    <description>Recent content on Realjf&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 23 Apr 2020 17:31:45 +0800</lastBuildDate>
    
	<atom:link href="https://realjf.io/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>TCP协议流量控制与拥塞控制详解</title>
      <link>https://realjf.io/network/tcp-protocol/</link>
      <pubDate>Thu, 23 Apr 2020 17:31:45 +0800</pubDate>
      
      <guid>https://realjf.io/network/tcp-protocol/</guid>
      <description>TCP的主要特点  面向连接的运输层协议 可靠交付服务 提供全双工通信 面向字节流  连续ARQ协议  连续ARQ协议规定：发送方维持一个发送窗口，每收到一个确认，就把发送窗口向前滑动一个分组的位置。 接收方采用累积确认的方式，在收到几个分组后，对按序到达的最后一个分组发送确认。   MSS最大报文段长度
 滑动窗口协议 以字节为单位的滑动窗口。每个tcp活动连接的两端都维护一个发送窗口结构和接收窗口结构。tcp以字节为单位维护其窗口结构。 随着时间推移，当接收到返回的数据ack，滑动窗口也随之右移。
每个tcp报文段都包含ack号和窗口通告信息，tcp发送端可以据此调节窗口结构。
流量控制 所谓流量控制，就是让发送方的发送速率不要太快，要让接收方来得及接收，利用滑动窗口机制可以很方便在tcp连接上实现对发送方的流量控制。
图例说明下 TCP报文段发送机制  第一种机制是TCP维持一个tcp报文段发送出去 第二种机制是由发送方的应用进程指明要求发送报文段 第三种机制是发送方的一个计时器期限到了，这时就把当前已有的缓存数据装入报文段发送出去。  拥塞控制 拥塞控制原理 所谓拥塞控制就是防止过多的数据注入到网络中，这样就可以使网络中的路由器或链路不致过载。拥塞控制索要做的都有一个前提， 就是网络能够承受现有的网络负荷。拥塞控制是一个全局性的过程，涉及所有的主机、所有的路由器，以及与降低网络传输性能有关的所有因素。
流量控制往往是指点对点通信量的控制。
拥塞控制方法 拥塞控制是一个动态的问题，从大的方面看，可以分为开环控制和闭环控制两种方法。
开环控制 就是在设计网络时事先将有关发生拥塞的因素考虑周到，力求网络在工作时不产生拥塞。
闭环控制 闭环控制基于反馈环路，主要有以下几种措施：
 监测网络系统以便检测到拥塞在何时、何处发生。 把拥塞发生的信息传送到可采取行动的地方 调整网络系统的运行以解决出现的问题  拥塞控制的算法 tcp进行拥塞控制的算法有四种，即慢开始(slow-start)、拥塞避免(congestion avoidance)、快重传(fast retransmit)和快恢复(fast recovery)
慢开始和拥塞避免  发送方让自己的发送窗口等于拥塞窗口 判断网络出现拥塞的依据就是出现了超时
 慢开始算法思路：当主机开始发送数据时，由于并不清楚网络的负荷情况，所以如果立即把大量数据字节注入到网络，那么就有可能引起网络发生拥塞。 经验证明，较好的方法是先探测一下，即由小到大逐渐增大发送窗口，也就是说，由小到大逐渐增大拥塞窗口数值。
RFC5681规定初始拥塞窗口cwnd设置为不超过2至4个SMSS（最大报文段）的数值，具体如下：
 若SMSS&amp;gt;2190字节，则设置初始拥塞窗口cwnd=2xSMSS字节，且不得超过2个报文段。 若SMSS&amp;gt;1095且SMSS&amp;lt;=2190字节，则设置初始拥塞窗口cwnd=3xSMSS字节，且不得超过3个报文段。 若SMSS&amp;lt;=1095字节，则设置初始拥塞窗口cwnd=4xSMSS字节，且不得超过4个报文段。  慢开始规定：在每收到一个对新的报文段的确认后，可以把拥塞窗口增加最多一个SMSS的数值。即
拥塞窗口cwnd每次的增加量 = min(N,SMSS)
这里使用报文段的个数作为窗口大小的单位，来阐述拥塞控制原理
 因此，使用慢开始算法后，每经过一个传输轮次，拥塞窗口cwnd就加倍。</description>
    </item>
    
    <item>
      <title>Tcp 连接的建立与终止</title>
      <link>https://realjf.io/network/tcp/</link>
      <pubDate>Thu, 23 Apr 2020 16:10:57 +0800</pubDate>
      
      <guid>https://realjf.io/network/tcp/</guid>
      <description>tcp连接的建立和终止依赖于connect、accept、close等函数。同时tcp也可以说是全双工的方式进行通信。
TCP连接的建立：三次握手 准备条件 服务器通过调用socket、bind和listen三个函数完成准备监听工作，称为被动打开。
第一次 客户端通过调用connect发起连接建立请求，客户端通过tcp发送一个SYN（同步）数据包， 数据包中携带的是建立连接发送数据的初始序列号。通常SYN数据包不携带数据
第二次 服务器收到客户端的SYN数据包后，需要对这个数据包回应一个ACK数据包（其确认序列号=初始序列号+1），且自己也需要发送建立连接的 同步SYN数据包，服务器在一个数据包中发送SYN和ACK信息，并携带自己的初始序列号
第三次 客户端确认收到服务端发送的ACK后，需要回应服务端发送的SYN并发送一个ACK数据包（其确认序列号=初始序列号+1）给服务端。 服务端收到后，连接建立。
TCP连接的终止： 四次挥手 第一次 某个进程首先调用close，称为主动关闭，主动端发送一个FIN数据包，同样携带一个初始序列号，表示数据发送完毕。
第二次 接收这个FIN数据包的称为被动端，它的接收也作为一个文件结束符传递给接收端应用程序，并发送一个ACK数据包给主动端，且携带一个确认序列号
第三次 过了某个时间，被动端的应用程序处理了这个文件结束符，调用了close关闭这端的套接字，所以也发送一个FIN数据包并携带一个初始序列号过去。
第四次 主动端收到关闭请求，也需要回应一个ACK数据包并携带一个确认序列号过去，表示结束。
TCP 状态转换图 TIME_WAIT状态 TIME_WAIT状态，在主动关闭端最后发送确认关闭ACK数据包后，需要等待一段时间才能关闭。这个停留时间是最长数据包生命周期（maximum segment lifetime）的两倍，称为2MSL。
任何TCP实现都必须为MSL选择一个值，RFC1122建议是2分钟。不过伯克利套接字实现改用30秒，这意味着持续时间可能在一分钟到4分钟之间。
TIME_WAIT存在的理由：
 可靠实现tcp全双工连接的终止 允许老的重复数据包在网络中消失（每个数据包都有一个跳数ttl限制，通常是255）  第一个理由 因为假设最后一个ACK数据包可能丢失，这样被动端在没有收到FIN的ACK确认关闭数据包时，会启用超时重传，重新发送FIN数据包， 因此，主动端必须维持状态以等待重传的那个FIN数据包，并允许它重新发送确认ACK数据包。
第二个理由 假设某个套接字（这里指ip和端口的组合）刚关闭，过一段时间这个套接字上又建立了另一个连接，如果这个时候那个丢失的数据包出现， 则可能被这个新连接误认为是发给它的数据，造成错误。tcp为了防止这种问题出现，就必须让这个套接字上之前关闭的连接等待一段时间， 以等待这个丢失的数据包消失，而这个时间就是2MSL，这个时间足以让这个丢失的数据包在网络中消失。</description>
    </item>
    
    <item>
      <title>C&#43;&#43;常用关键字用法解析 const static volatile extern mutable</title>
      <link>https://realjf.io/cpp/keyword/</link>
      <pubDate>Wed, 22 Apr 2020 18:09:41 +0800</pubDate>
      
      <guid>https://realjf.io/cpp/keyword/</guid>
      <description>static 修饰局部变量  静态局部变量只作用于其定义的函数期间，函数结束，其所占用的内存空间也被回收。 在静态存储区分配空间，只初始化一次  修饰全局变量  也称静态全局变量，其作用域在定义它的文件里，不能作用于其他文件。 静态全局变量在静态存储区分配空间，在程序开始运行时完成初始化，也是唯一的一次初始化  修饰函数  静态函数只在声明它的文件中可见，不能被其他文件使用。  修饰类成员  对于静态类成员，它属于类，而不属于某个对象实例，多个对象之间共享静态类成员 静态类成员存储于静态存储区，生命周期为整个程序执行期 静态类成员需要初始化，且在类外初始化，默认初始化为0  初始化方法：&amp;lt;数据类型&amp;gt; &amp;lt;类名&amp;gt;::&amp;lt;静态类成员&amp;gt;=&amp;lt;值&amp;gt;
修饰类成员函数  同样静态类成员函数属于整个类，而非某个实例对象，也没有this指针，需要通过类名进行访问。 不能将静态类成员函数定义为虚函数 &amp;gt; 虚函数依赖vptr和vtable，vptr通过类的构造函数生成，且只能用this指针访问，这也就是为什么静态成员函数不能是虚函数的原因 由于静态成员函数没有this指针，所以就差不多等同于nonmember函数，结果就产生了一个意想不到的好处：成为一个callback函数，使得我们得以将C++和C-based X Window系统结合，同时也成功的应用于线程函数身上 为了防止父类的影响，可以在子类定义一个与父类相同的静态变量，以屏蔽父类的影响。  const 规则：const离谁近，谁就不能被修改，只读的意思，且需要初始化。
修饰基本数据类型  修饰一般常量时，可以在类型说明符前也可以在其后，只要在使用时不改变常量即可。 const修饰指针变量*及引用变量&amp;amp; &amp;gt; 如果const位于星号*的左侧，则const就是用来修饰指针所指向的变量，即指针指向为常量 &amp;gt; 如果const位于星号的右侧，const就是修饰指针本身，即指针本身是常量  作为函数参数的修饰符 用相应的变量初始化const常量，则在函数体中，按照const所修饰的部分进行常量化，保护了原对象的属性不被修改
void say(const char* str){...}  作为函数返回值的修饰符 声明了返回值后，对返回值起到保护作用，即使得其返回值不为“左值”，只能作为右值使用。
const int add(int a, int b){...}  const修饰类成员 修饰的类成员的初始化只能在类的构造函数的初始化表中进行
const修饰类成员函数 作用是修饰的成员函数不能修改类的任何成员变量
int funcA() const {}  const修饰类对象，定义常量对象 常量对象只能调用常量函数，别的成员函数都不能调用。</description>
    </item>
    
    <item>
      <title>zab协议 （Zookeeper Zab Protocol）</title>
      <link>https://realjf.io/distributed/zookeeper-zab-protocol/</link>
      <pubDate>Wed, 22 Apr 2020 09:18:44 +0800</pubDate>
      
      <guid>https://realjf.io/distributed/zookeeper-zab-protocol/</guid>
      <description>ZAB协议，（ZooKeeper Atomic Broadcast, ZooKeeper原子消息广播协议） ZAB协议不像Paxos算法，它是一种特别为Zookeeper设计的崩溃可恢复的原子消息广播协议
ZAB协议的核心 所有事务请求必须由一个全局唯一的服务器来协调处理，这样的服务器被称为Leader服务器，而余下的其他服务器则成为follower服务器， leader服务器负责将一个客户端事务请求转换成一个事务Proposal，并将该Proposal分发给集群中所有的follower服务器，之后leader服务器需要 等待所有follower服务器的反馈，一旦超过半数的follower服务器进行了正确的反馈后，那么leader就会再次向所有的follower服务器分发commit消息， 要求其将前一个Proposal进行提交。
ZAB协议内容 ZAB协议包括两种基本模式：崩溃恢复和消息广播。
当整个服务框架在启动过程中，或是当leader服务器出现网络中断、崩溃退出与重启等异常情况时，ZAB协议就会进入恢复模式并选举产生新的leader服务器。 当选举产生新的leader服务器后，同时集群中已经有过半机器与该leader服务器完成了状态同步之后，ZAB协议就会退出恢复模式，其中，所谓的状态同步是指数据同步， 用来保证集群中存在过半的机器能够和leader服务器的数据状态保持一致。
当集群中已经有过半的follower服务器完成了和leader服务器的状态同步，那么整个服务器框架就可以进入消息广播模式了。当一台同样遵循ZAB协议的服务器启动后加入到集群中， 如果此时集群中已经存在一个leader服务器在负责进行消息广播，那么新加入的服务器就会自觉的进入数据恢复模式：找到leader所在的服务器，并与其进行数据同步， 然后一起参与到消息广播流程中。
ZooKeeper设计成只允许唯一的一个leader服务器来进行事务请求的处理。leader服务器在接收到客户端的事务请求后，会生成对应的事务提案并发起一轮广播协议， 而如果集群中的其他机器接收到客户端的事务请求，那么这些非leader服务器会首先将这个事务请求转发给leader服务器。
当leader服务器出现崩溃退出或机器重启，亦或集群中已经不存在过半的服务器与该leader服务器保持正常通信时，那么在重新开始新一轮的原子广播事务操作之前， 所有进程首先会使用崩溃恢复协议来使彼此达到一个一致的状态，于是整个ZAB流程就会从消息广播模式进入到崩溃恢复模式。
数据同步 在ZAB协议的事务编号ZXID设计中，ZXID是一个64位的数字，其中低32位可以看作是一个简单的单调递增的计数器，针对客户端的每一个事务请求，leader服务器在产生 一个新的事务Proposal的时候，都会对该计数器进行加1操作，而高32位则代表了leader周期epoch的编号，每当选举产生一个新的leader服务器，就会从这个leader 服务器上取出其本地日志中最大事务Proposal的ZXID，并从该ZXID中解析出对应的epoch值，然后再对其进行加1操作，之后就会以此编号作为新的epoch，并 将低32位置0来开始生成新的ZXID。ZAB协议中的这一通过epoch编号来区分leader周期变化的策略，能够有效避免不同的leader服务器错误地使用相同的ZXID编号 提出不一样的事务Proposal的异常情况，这对于识别在leader崩溃恢复前后生成的Proposal非常有帮助。
基于这样的策略，当一个包含了上一个leader周期中尚未提交过的事务Proposal的服务器启动时，其肯定无法成为leader，因为当前集群中一定包含一个Quorum集合， 该集合中的机器一定包含了更高epoch的事务Proposal，因此这台机器的事务Proposal肯定不是最高，也就无法成为leader了。
ZAB与Paxos算法的联系与区别 联系  两者都存在一个类似leader进程的角色，由其负责协调多个follower进程的运行 leader进程都会等待超过半数的follower做出正确的反馈后，才会将一个提案进行提交 在ZAB协议中，每个Proposal中都包含了一个epoch值，用来代表当前leader周期，在Paxos算法中，同样存在这样一个标识，只是名字是Ballot。
区别 Paxos算法一个新选举产生的主进程会进行两个阶段的工作，第一阶段称为读阶段，与所有其他进程通信收集上一个主进程提出的提案，并将它们提交。 第二个阶段称为写阶段，主进程开始提出自己的提案。
  ZAB协议在Paxos算法上额外添加了一个同步阶段。在同步阶段之前，ZAB有个类似Paxos算法的读阶段，称为发现阶段。同步阶段之后，也有一个类似的写阶段。</description>
    </item>
    
    <item>
      <title>分布式一致性协议 2PC和3PC Paxos（Distributed Consistency Protocol）</title>
      <link>https://realjf.io/distributed/distributed-consistency-protocol/</link>
      <pubDate>Tue, 21 Apr 2020 10:05:32 +0800</pubDate>
      
      <guid>https://realjf.io/distributed/distributed-consistency-protocol/</guid>
      <description>分布式一致性协议在实践过程中产生了许多优秀的协议和算法，其中就包括两阶段提交、三阶段提交协议和Paxos算法。
2PC：两阶段提交 两阶段提交，主要由协调者和参与者组成，协调者负责协调所有参与者是否提交最后结果，并保证各参与者之间的结果一致（提交或者回滚）。
阶段一：提交事务请求阶段  协调者向所有参与者发送事务内容，询问是否可以执行事务提交操作，并等待各参与者的回应 各参与者节点执行事务操作，并将undo和redo信息记录事务日志中 如果参与者执行了事务操作，那么就反馈给协调者yes响应，表示事务可以执行，否则返回no，表示事务不可以执行。  阶段二：执行事务提交阶段 阶段二主要是对各参与者反馈的情况决定是否继续进行事务提交操作，或者回滚。 主要包括两种情况：
第一种：执行事务提交 假如协调者从所有的参与者获得的反馈都是yes，那么就执行事务提交。
 发送提交请求，协调者向所有参与者节点发送commit请求 事务提交，参与者接收到commit请求后，会正式执行事务提交，并在完成后释放整个事务执行期间占用的资源 反馈事务提交结果，提交完成后，向协调者发送ack信息 完成事务，协调者接收到所有参与者的ack信息后，完成事务。  第二种：中断事务 假如任何一个参与者向协调者反馈了no，或者在等待超时之后，协调者仍然没有接收到所有参与者的反馈，那么就中断事务。
 发送回滚请求，协调者向所有参与者节点发送rollback请求 事务回滚，参与者接收到rollback请求后，利用第一阶段中记录的undo信息来执行事务回滚，并在完成回滚后释放在整个事务期间占用的资源 反馈事务回滚结果，参与者在完成事务回滚之后，向协调者发送ack信息 中断事务，协调者接收到所有的参与者反馈的ack信息后，完成事务中断  两阶段提交优缺点  优点：原理简单，实现方便 缺点：同步阻塞，单点问题、脑裂、容错机制简单  3PC：三阶段提交 三阶段提交可说是2PC的改进版，其将二阶段提交协议的提交事务请求过程一分为二，形成了CanCommit、PreCommit和do Commit三个阶段组成的事务处理协议。
阶段一：CanCommit  事务询问，协调者向所有的参与者发送一个包含事务内容的canCommit请求，询问是否可以执行事务提交操作，并开始等待各参与者的响应。 各参与者向协调者反馈事务询问的响应，参与者在接收到来自协调者的canCommit请求后，正常情况是，如果其自身认为可以顺利执行事务，那么会反馈yes，并进入预备状态，否则反馈no  阶段二：PreCommit 阶段二，协调者会根据各参与者的反馈情况来决定是否可以进行事务的PreCommit操作，正常情况，包括两种：
第一种：执行事务预提交 假如协调者从所有的参与者获得的反馈都是yes，那么就会执行事务预提交。
 发送预提交请求，协调者向所有参与者节点发送preCommit请求，并进入prepared阶段。 事务预提交，参与者接收到preCommit请求后，会执行事务操作，并将undo和redo信息记录到事务日志中。 各参与者向协调者反馈事务执行的响应，如果参与者成功执行了事务操作，那么就反馈给协调者ack响应，同时等待最终指令：提交（commit）或终止（abort）  第二种：中断事务 如果协调者收到任何一个参与者反馈了no，或者等待超时之后，仍然无法接收到所有参与者的反馈，那么就中断事务。
 发送中断请求，协调者向所有参与者节点发送abort请求 中断事务，无论是收到来自协调者的abort，或者是等待协调者请求过程中出现超时，参与者都会中断事务。  阶段三：doCommit 这个阶段是真正执行事务提交，存在两种可能
第一种：执行提交  发送提交请求，假设协调者处于正常状态，并且收到了所有参与者的ack信息，那么它就从预提交状态转换到提交状态，并向所有参与者发送doCommit请求 事务提交，参与者接收到doCommit请求后，会正式执行事务提交操作，并在完成提交之后释放在整个事务执行期间占用的资源。 反馈事务提交结果，参与者在完成事务提交之后，向协调者发送ack信息 完成事务，协调者接收到所有参与者的反馈ack信息后，完成事务。  第二种：中断事务 在这个阶段，假设协调者正常，并且有任意一个参与者反馈了no，或者等待超时之后，协调者无法接收到所有参与者的响应，那么就中断事务。
 发送中断请求，协调者向所有参与者节点发送abort请求 事务回滚，参与者接收到abort请求后，会利用其在阶段二中记录的undo信息来执行回滚，并在完成回滚之后释放事务执行期间占用的资源。 反馈事务回滚结果，事务完成回滚之后，向协调者发送ack信息 中断事务，协调者接收到所有参与者反馈的的ack信息后，中断事务  注意：一旦进入阶段三，可能有两种故障</description>
    </item>
    
    <item>
      <title>五种I/O模式 （Io Pattern）</title>
      <link>https://realjf.io/cpp/io-pattern/</link>
      <pubDate>Fri, 17 Apr 2020 15:22:29 +0800</pubDate>
      
      <guid>https://realjf.io/cpp/io-pattern/</guid>
      <description>常见的五种I/O模式 I/O模式有这五种，分别是：
 阻塞I/O （linux下默认都采用阻塞I/O） 非阻塞I/O （可以通过fcntl或者open设置使用O_NONBLOCK参数，将文件描述符设置为非阻塞） I/O多路复用 信号驱动I/O 异步I/O  其中前面四种被称为同步IO
用户空间与内核空间 首先理解，当进程运行在内核空间时就处于内核态，而进程运行在用户空间时则处于用户态。 在内核态下，进程运行在内核地址空间中，此时的 CPU 可以执行任何指令。运行的代码也不受任何的限制，可以自由地访问任何有效地址，也可以直接进行端口的访问。 在用户态下，进程运行在用户地址空间中，被执行的代码要受到 CPU 的诸多检查，它们只能访问映射其地址空间的页表项中规定的在用户态下可访问页面的虚拟地址，且只能对任务状态段(TSS)中 I/O 许可位图(I/O Permission Bitmap)中规定的可访问端口进行直接访问。
所以，区分内核空间和用户空间本质上是要提高操作系统的稳定性及可用性
进程切换过程 从一个进程的运行转到另一个进程上运行，这个过程中经过下面这些变化： - 保存上下文，包括程序计数器和其他寄存器 - 更新PCB信息(进程管理与控制信息) - 把进程pcb加入等待挂起等队列 - 选择另一个进程执行，并更新其pcb - 更新内存管理的数据结构 - 恢复上下文
阻塞IO 同步阻塞IO，用户进程发起一个IO请求，内核查看数据是否就绪，如果没有，就等待数据就绪，而用户进程处于阻塞状态， 且交出cpu控制权，但数据就绪后，内核将数据拷贝到用户进程空间，并通知用户进程，用户进程解除阻塞状态，进入就绪状态，等待下一次运行。
非阻塞I/O 非阻塞IO，用户进程发起IO请求后，内核检查相应状态，无论就绪与否都返回结果给用户进程，用户进程无需等待就可以根据相应结果进行处理， 当然用户进程可以循环发起IO请求操作，这相当于一直占用CPU。
I/O多路复用 多路IO复用是目前比较多的用于环节C10K问题的方案，采用select、poll、epoll等方式，其中epoll是linux特有的。 相比较非阻塞IO，多路复用的效率明显要高，且是在内核中进行的。
下面分别简要说下select、poll和epoll的区别
select select 函数监听的文件描述符有三类，writefds、readfds和exceptfds，调用后select会阻塞进程，直到有描述符就绪，或者超时， 函数返回后，通过遍历fdset，查找相应就绪的描述符进行处理。
select目前支持几乎所有的平台，在linux上一般限制最大监视文件描述符大小为1024。
 select最大限制是单进程fd最大支持1024个，64为系统默认为2048 对文件描述符采用轮询，效率低 需要维护一个用于存放大量fd的数据结构  poll poll本质上与select类似，管理多个文件描述符，也是进行轮询，根据描述符的状态进行处理。 但它没有最大数限制，poll也有个致命缺陷，包含大量文件描述符的数组被整个在内核与用户空间之间多次复制， 开销随着文件描述符数量激增
epoll epoll是linux2.6开始提供的功能，是对poll的改进，epoll没有文件描述符限制，使用一个文件描述符管理多个描述符， 将用户关心的事件描述符映射到内核中，期间只复制一次。
epoll使用epoll_ctl注册文件描述符，并监听自己感兴趣的事件，使用epoll_wait可以收到事件通知。
epoll的两种触发模式  EPOLLLT （水平触发）当epoll_wait监听的事件发生时，将此事件通知用户进程，用户进程可以不立即处理该事件。下次调用epoll_wait时，会再次响应并通知此事件 EPOLLET （边缘触发）当epoll_wait监听的事件发生时，将此事件通知用户进程，用户进程必须立即处理该事件。如果不处理，下次调用epoll_wait时，不会再次响应通知此事件。  epoll的优点  没有最大并发数限制 效率提升，内核态监听事件，只复制一次事件映射集，不是轮询机制，而是使用事件通知机制，只有活跃的文件描述符才占用开销。  epoll的工作流程 信号驱动I/O 信号驱动IO,用户进程首先需要安装SIGIO信号处理函数，然后内核等待IO请求，用户进程继续执行， 直到内核发出SIGIO信号，表示数据准备好，并拷贝到用户进程空间，用户线程接收到信号之后，便在信号函数中调用IO读写操作来进行实际的IO请求操作。</description>
    </item>
    
    <item>
      <title>C&#43;&#43;多态与虚函数 （Polymorphism）</title>
      <link>https://realjf.io/cpp/polymorphism/</link>
      <pubDate>Fri, 17 Apr 2020 14:04:30 +0800</pubDate>
      
      <guid>https://realjf.io/cpp/polymorphism/</guid>
      <description>什么是多态？ C++的多态即针对同一事物对不同场景表现多种形态，称为c++的多态性
多态分为静态多态和动态多态 - 静态多态又分为函数重载和泛型编程 - 动态多态则通过虚函数实现
多态的作用  提供了接口与具体实现之间的另一层隔离， 改善了代码的组织结构和可读性以及可扩展性  静态多态 直接上代码
int Add(int a, int b) { return a + b; } double Add(float a, float b) { return a + b; } // 调用的时候 int main() { Add(1, 2); // 调用的是第一个Add Add(1.5, 2.5); // 调用的是第二个Add return 0; }  可以看到，静态多态是在编译期间可以确定的，根据具体的了类型调用不同的函数
动态多态 首先要理解，这里的动态是指在程序运行期间，所以动态多态只能在程序运行的时候确定。
而要实现动态多态，这里需要用到关键字virtual，声明一个函数为虚函数
具体代码：
class Animal { public: virtual void Say() = 0; } class Cow : public Animal { public: void Say() { cout &amp;lt;&amp;lt; &amp;quot;哞哞&amp;quot; &amp;lt;&amp;lt; endl; } } class Sheep : public Animal { public: void Say() { cout &amp;lt;&amp;lt; &amp;quot;咩咩&amp;quot; &amp;lt;&amp;lt; endl; } } // 开始使用 int main() { Animal* cow = (Animal*)new Cow(); Animal* sheep = (Animal*)new Sheep(); cow-&amp;gt;Say(); sheep-&amp;gt;Say(); }  有上述代码可以看出，多态是基类中包含虚函数，而子类对其进行重写的，并且通过基类对象的指针或引用调用虚函数形成多态。</description>
    </item>
    
    <item>
      <title>C&#43;&#43;智能指针详解（Smart Pointer）</title>
      <link>https://realjf.io/cpp/smart-pointer/</link>
      <pubDate>Fri, 17 Apr 2020 11:21:57 +0800</pubDate>
      
      <guid>https://realjf.io/cpp/smart-pointer/</guid>
      <description> 智能指针 智能指针在C++11版本之后提供，包含在头文件中，包括三种： - shared_ptr - unique_ptr - weak_ptr
智能指针的作用 由于C++没有垃圾回收机制，一切内存堆操作都是程序员自己管理，但对于程序员来说管理堆不胜麻烦，稍有不慎忘记释放就会造成内存泄露最终导致内存溢出等问题。 而智能指针则能有效避免此类问题发生。
智能指针通过对普通指针进行类封装，使其表现的跟普通指针类似的行为。
shared_ptr指针 shared_ptr 使用引用计数，每一个shared_ptr的拷贝都指向相同的内存地址，每使用一次，内部的引用计数加1， 每析构一次，内部的引用计数减1，减到0时，自动删除所指向的堆内存。shared_ptr内部的引用计数是线程安全的，但是对象的读取需要加锁。
 初始化。std::shared_ptr n，也可以make_shared函数初始化。不能直接赋值一个指针，因为它是类。 拷贝和赋值，拷贝引用计数加1，赋值引用计数减1，当计数为0时，自动释放内存。 get函数获取原始指针 不要用一个原始指针初始化多个shared_ptr，否则会造成二次释放同一内存。 避免循环引用，循环引用会导致内存泄漏。  unique_ptr指针 unique_ptr 唯一拥有其所指对象，统一时刻只能有一个unique_ptr指向给定对象（通过禁止拷贝语义，只有移动语义实现）。 相比原始指针，unique_ptr的RAII特性，使得其在出现异常时，能自动释放指向对象占用资源。unique_ptr生命周期从创建到作用域结束， 离开作用域时，若其指向对象，则将其所指向对象销毁。
unique_ptr在生命周期内，可以改变智能指针所指对象，通过release释放所有权，通过reset函数指定新对象，通过移动语义转移所有权。
weak_ptr指针  weak_ptr作为一个辅助智能指针，配合shared_ptr可以对资源使用情况进行观测。 weak_ptr可以从一个shared_ptr或另一个weak_ptr对象中构造，以获得资源观测权，它不会使原对象引用计数增加，  智能指针的原理 智能指针：实际指行为类似于指针的类对象，是利用了一种叫做RAII（资源获取即初始化）的技术对普通的指针进行封装, 它的一种通用实现方法是采用引用计数的方法。
 1.智能指针将一个计数器与类指向的对象相关联，引用计数跟踪共有多少个类对象共享同一指针。 2.每次创建类的新对象时，初始化指针并将引用计数置为1； 3.当对象作为另一对象的副本而创建时，拷贝构造函数拷贝指针并增加与之相应的引用计数； 4.对一个对象进行赋值时，赋值操作符减少左操作数所指对象的引用计数（如果引用计数为减至0，则删除对象），并增加右操作数所指对象的引用计数；这是因为左侧的指针指向了右侧指针所指向的对象，因此右指针所指向的对象的引用计数+1； 5.调用析构函数时，构造函数减少引用计数（如果引用计数减至0，则删除基础对象）。 6.实现智能指针有两种经典策略：一是引入辅助类，二是使用句柄类。这里主要讲一下引入辅助类的方法  </description>
    </item>
    
    <item>
      <title>死锁 Deadlock</title>
      <link>https://realjf.io/posts/deadlock/</link>
      <pubDate>Thu, 16 Apr 2020 15:28:20 +0800</pubDate>
      
      <guid>https://realjf.io/posts/deadlock/</guid>
      <description> 什么是死锁？ 简单说，是指两个或两个以上的线程在执行过程中，彼此持有对方需要的资源和处于等待对方释放资源的现象， 如果没有外力作用，这种状态将一直持续下去。
如何避免？ 避免死锁的一般建议是：对竞争资源按顺序采用互斥加锁
当然，如果能在编程时就注意这方便的问题，将可以用更好的方式，比如：
 避免嵌套锁 避免在持有锁时调用用户提供的代码 使用固定顺序获取锁 使用锁的层次结构  </description>
    </item>
    
    <item>
      <title>memcache数据提前过期（丢失）Memcache Data Lost</title>
      <link>https://realjf.io/mc/memcache-data-lost/</link>
      <pubDate>Wed, 15 Apr 2020 10:51:20 +0800</pubDate>
      
      <guid>https://realjf.io/mc/memcache-data-lost/</guid>
      <description>背景 今天遇到一个比较奇葩的问题，使用脚本测试接口防洪攻击时，mc的封禁数据还未到过期时间就出现数据“丢失”的情况， 一直以为是代码问题，后来偶然想到memcache在达到内存超过50%以上时，就可能采用LRU算法回收部分内存，考虑到防洪封禁数据 比较多，所以做了本地测试
了解下memcache的一些状态信息 php通过getStat函数获取memcache状态信息。
 pid mc进程号 uptime 服务器已运行秒数 version 版本 time 当前时间 libevent libevent版本 pointer_size 当前os的指针大小(64位系统一般为64) rusage_user 进程的累计用户时间 rusage_system 进程的累计系统时间 curr_connections 服务器当前打开的连接数 total_connections 从服务器启动后累计打开的总连接数 connection_structures 服务器分配的连接结构数 reserved_fds cmd_get get命令总请求次数 cmd_set set命令总请求次数 cmd_flush flush命令请求次数 cmd_touch touch命令请求次数 get_hits get命令总命中次数 get_misses get命令总未命中次数 delete_misses delete_hits incr_misses incr_hits decr_misses decr_hits cas_misses cas_hits cas_badval 使用擦拭次数 touch_hits touch_misses auth_cmds 认证命令处理次数 auth_errors 认证失败次数 bytes_read 总读取字节数（请求字节数） byte_written 总发送字节数（结果字节数） limit_maxbytes 分配给memcache的内存大小（字节） accepting_conns 服务器是否大打过最大连接数 listen_disabled_num 失效的监听数 threads 当前线程数 conn_yields 连接操作主动放弃数目 hash_power_level hash_bytes hash_is_expanding malloc_fails bytes 当前存储内容所占总字节数 curr_items 当前存储的items数量 total_items 从启动后存储的items总数量 expired_unfetched evicted_unfetched evictions 为获取空闲内存而删除的items数，LRU算法释放（分配给memcache的空间用满后需要删除旧的items来得到空间分配给新的items） reclaimed 已过期的数据条目来存储新数据的数目 crawler_reclaimed lrutail_reflocked  解决方法是，增大MC使用内存</description>
    </item>
    
    <item>
      <title>大文件分片上传 之 基于webuploader组件（Chunk Upload File）</title>
      <link>https://realjf.io/php/chunk-upload-file/</link>
      <pubDate>Mon, 13 Apr 2020 14:00:07 +0800</pubDate>
      
      <guid>https://realjf.io/php/chunk-upload-file/</guid>
      <description>针对大文件（上百兆或者好几个G的大文件上传，总是比较麻烦的，这里将介绍一个比较方便的解决方案
准备  百度的webuploader组件 lnmp或lamp开发环境  本次使用的是百度分享的分片js组件webuploader
同时后端使用php接收分片文件，并进行最后的组装。
第一步，首先下载webuploader插件 下载地址：https://github.com/fex-team/webuploader/releases
解压后文件结构如下：
├── Uploader.swf // SWF文件，当使用Flash运行时需要引入。 ├── webuploader.js // 完全版本。 ├── webuploader.min.js // min版本 ├── webuploader.custom.js ├── webuploader.nolog.js ├── webuploader.flashonly.js // 只有Flash实现的版本。 ├── webuploader.flashonly.min.js // min版本 ├── webuploader.html5only.js // 只有Html5实现的版本。 ├── webuploader.html5only.min.js // min版本 ├── webuploader.withoutimage.js // 去除图片处理的版本，包括HTML5和FLASH. └── webuploader.withoutimage.min.js // min版本 下载  第二步，创建一个html页面，引入一下文件 &amp;lt;link href=&amp;quot;/resource/webuploader/webuploader.css&amp;quot; rel=&amp;quot;stylesheet&amp;quot; /&amp;gt; &amp;lt;script src=&amp;quot;/resource/webuploader/webuploader.js&amp;quot;&amp;gt;&amp;lt;/script&amp;gt;  页面内容如下：
&amp;lt;div id=&amp;quot;uploader&amp;quot; class=&amp;quot;wu-example&amp;quot;&amp;gt; &amp;lt;div id=&amp;quot;uploader&amp;quot; class=&amp;quot;wu-example&amp;quot;&amp;gt; &amp;lt;!--用来存放文件信息--&amp;gt; &amp;lt;div class=&amp;quot;filename&amp;quot;&amp;gt;&amp;lt;/div&amp;gt; &amp;lt;div class=&amp;quot;state&amp;quot;&amp;gt;&amp;lt;/div&amp;gt; &amp;lt;div class=&amp;quot;progress&amp;quot;&amp;gt; &amp;lt;div id=&amp;quot;progress_bar&amp;quot; class=&amp;quot;progress-bar progress-bar-info progress-striped active&amp;quot; role=&amp;quot;progressbar&amp;quot; style=&amp;quot;width: 0%&amp;quot;&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;div class=&amp;quot;btns&amp;quot;&amp;gt; &amp;lt;div id=&amp;quot;picker&amp;quot;&amp;gt;选择文件&amp;lt;/div&amp;gt; &amp;lt;button id=&amp;quot;ctlBtn&amp;quot; class=&amp;quot;btn btn-default&amp;quot;&amp;gt;开始上传&amp;lt;/button&amp;gt; &amp;lt;button id=&amp;quot;pause&amp;quot; class=&amp;quot;btn btn-danger&amp;quot;&amp;gt;暂停上传&amp;lt;/button&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;/div&amp;gt; &amp;lt;/div&amp;gt;  第三步，js逻辑如下 &amp;lt;script type=&amp;quot;text/javascript&amp;quot;&amp;gt; $(function () { var GUID = WebUploader.</description>
    </item>
    
    <item>
      <title>C&#43;&#43; 的Struct和Class 的区别</title>
      <link>https://realjf.io/cpp/struct-and-class-inherit/</link>
      <pubDate>Sat, 22 Feb 2020 22:14:22 +0800</pubDate>
      
      <guid>https://realjf.io/cpp/struct-and-class-inherit/</guid>
      <description> 关于c++的class和struct的不同可以简单归纳为以下几点： 内部成员变量及成员函数的默认防控属性不同 struct默认防控属性是public，而class默认的防控属性是Private
继承关系中的默认防控属性的区别 在继承关系中，struct默认是public，而class是private
在继承中的基类和子类之间的继承方式
   继承方式 基类的public成员 基类的protected成员 基类中的private成员     public继承 仍为public成员 仍为protected成员 不可见   protected继承 变为protected成员 变为protected成员 不可见   private继承 变为private成员 变为private成员 不可见    模板中使用 class关键字可以用于定义模板参数，但是struct不行
template&amp;lt;template T, class Y&amp;gt; int Func(const T&amp;amp; t, const Y&amp;amp; y) { ... }  使用花括号{}赋值问题  struct如果没有定义构造函数，可以使用花括号对struct成员进行赋值。 struct中如果定义了一个构造函数，则不能使用花括号进行赋值  </description>
    </item>
    
    <item>
      <title>debian 系统启动进入Busybox Initramfs界面</title>
      <link>https://realjf.io/linux/error/boot-into-busybox-initramfs/</link>
      <pubDate>Thu, 20 Feb 2020 22:07:19 +0800</pubDate>
      
      <guid>https://realjf.io/linux/error/boot-into-busybox-initramfs/</guid>
      <description>首先说下背景  系统环境： debian 9  问题描述 今天使用vmware workstation的时候，提示操作失败，且提示为文件系统只读。 奇怪？怎么突然进入可读了，猜想可能文件系统哪里损坏导致进入只读保护模式。
所以重新启动，之后进入了busybox界面的Initramfs界面，输入help可以查看相应命令。 我使用exit直接退出看能否重新进入，发现还是提示错误，无法进入
 busybox可以提供一个比较完善的shell工具集以及运行环境，同时可以引导程序进入系统。
 解决 在多次尝试重启无果后，重新查看错误提示，提到了/dev/mapper/realjf&amp;ndash;vg-root的文件系统， 可能是文件系统损坏了，所以开始检查修复文件系统：fsck /dev/mapper/realjf&amp;ndash;vg-root， 然后系统开始检查文件系统损坏情况，并尝试进行修复，多次输入&amp;rsquo;y&amp;rsquo;后，提示文件系统修复完成， 然后重新输入exit看是否能重新进入系统，发现已经可以进入系统了。</description>
    </item>
    
    <item>
      <title>Channel 底层实现原理</title>
      <link>https://realjf.io/golang/channel-implement/</link>
      <pubDate>Mon, 20 Jan 2020 09:08:15 +0800</pubDate>
      
      <guid>https://realjf.io/golang/channel-implement/</guid>
      <description>channel是golang的一大特色，golang的goroutine之间的通信也建议通过channel机制实现。 那么我们有必要探讨下，channel的底层实现机制，以便我们更好的应用channel。
 本次探讨版本为go v1.13
 channel的实现原理 go中实现channel的文件包含在/runtime/chan.go中
type hchan struct { qcount uint // total data in the queue dataqsiz uint // size of the circular queue buf unsafe.Pointer // points to an array of dataqsiz elements elemsize uint16 closed uint32 elemtype *_type // element type sendx uint // send index recvx uint // receive index recvq waitq // list of recv waiters sendq waitq // list of send waiters // lock protects all fields in hchan, as well as several // fields in sudogs blocked on this channel.</description>
    </item>
    
    <item>
      <title>linux系统资源设置 之 Ulimit 命令</title>
      <link>https://realjf.io/linux/command/ulimit/</link>
      <pubDate>Tue, 10 Dec 2019 14:14:25 +0800</pubDate>
      
      <guid>https://realjf.io/linux/command/ulimit/</guid>
      <description>根据linux 开发手册， ulimit 设置和获取用户的资源限制
ulimit 参数说明
   选项 说明     -t 最大 cpu 占用时间 (单位是秒)   -f 进程创建文件大小的最大值 (单位是blocks)   -d 进程最大的数据段的大小，以kbytes为单位   -s 线程栈的大小，以kbytes为单位   -c 最大的core文件的大小，以blocks为单位   -m 最大内存大小，以kbytes为单位   -u 用户最大的可用的进程数   -n 可以打开的最大文件描述符数量   -l 最大可加锁内存大小，以kbytes为单位   -v 进程最大可用的虚拟内存，以kbytes为单位   -x    -i    -q    -e    -r    -N    -p 管道缓冲区的大小，以kbytes为单位   -a 显示所有资源限制的设定   -S 设定资源的弹性限制    </description>
    </item>
    
    <item>
      <title>如何写go语言的基准测试？</title>
      <link>https://realjf.io/golang/how-to-write-benchmarks-in-go/</link>
      <pubDate>Mon, 25 Nov 2019 15:08:36 +0800</pubDate>
      
      <guid>https://realjf.io/golang/how-to-write-benchmarks-in-go/</guid>
      <description>简介 Go标准库中test包包含一个基准测试工具，可用于检查Go代码的性能。 接下来将介绍如何使用测试包编写一个简单的基准测试。
一个基准测试示例 我们以斐波那契数列计算来做测试
func Fib(n int) int { if n &amp;lt; 2 { return n } return Fib(n-1) + Fib(n-2) }  创建一个名为*_test.go的测试文件，我们将对计算第20个斐波那契数列值进行性能测试。
func BenchmarkFib20(b *testing.B) { for n := 0; n &amp;lt; b.N; n++ { Fib(20) } }  编写基准测试与编写测试非常相似，因为它们共享测试包中的基础结构。一些关键区别是
 基准测试功能以Benchmark而不是Test开头 基准功能由测试包运行多次。 b.N的值每次都会增加，直到基准运行者对基准的稳定性感到满意为止。 每个基准测试必须执行b.N次测试代码。 BenchmarkFib20中的for循环将出现在每个基准测试函数中。  运行基准测试 我们可以使用go test -bench=. 调用基准测试
go test -bench=. # 运行结果如下 goos: linux goarch: amd64 pkg: test/benchmark BenchmarkFib-4 30000 44684 ns/op PASS ok test/benchmark 1.</description>
    </item>
    
    <item>
      <title>unix环境高级编程 之 apue.h环境安装配置 </title>
      <link>https://realjf.io/unix/how-to-setup-apue-env/</link>
      <pubDate>Sat, 23 Nov 2019 09:20:03 +0800</pubDate>
      
      <guid>https://realjf.io/unix/how-to-setup-apue-env/</guid>
      <description> 官网http://www.apuebook.com/apue3e.html
准备 apt-get install libbsd-dev  如果不执行上面步骤可能会出现如下问题：
barrier.c:(.text+0x6e): undefined reference to `heapsort’ collect2: ld make[1]: *** [barrier] make[1]: Leaving directory `/home/albert/Documents/progs/apue.3e/threads’ make: *** [all]  1. 下载解压 wget http://www.apuebook.com/src.3e.tar.gz tar zxvf src.3e.tar.gz cd apue.3e make  2. 复制相关头文件到/usr/include等 cp ./include/apue.h /usr/include cp ./lib/libapue.a /usr/local/lib  3. 搭建成功，测试 gcc 1-3.c -o 1-3 -lapue # 编译连接后 ./1-3 /lib # 查看是否正常执行程序  </description>
    </item>
    
    <item>
      <title>unix网络编程 之 myerr.h文件</title>
      <link>https://realjf.io/files/myerr/</link>
      <pubDate>Sat, 23 Nov 2019 08:51:45 +0800</pubDate>
      
      <guid>https://realjf.io/files/myerr/</guid>
      <description>unix网络编程 之 myerr.h文件 myerr.h
// myerr.h #include &amp;lt;errno.h&amp;gt;/* for definition of errno */ #include &amp;lt;stdarg.h&amp;gt;/* ISO C variable aruments */ static void err_doit(int, int, const char *, va_list); /* * Nonfatal error related to a system call. * Print a message and return. */ void err_ret(const char *fmt, ...) { va_list ap; va_start(ap, fmt); err_doit(1, errno, fmt, ap); va_end(ap); } /* * Fatal error related to a system call. * Print a message and terminate.</description>
    </item>
    
    <item>
      <title>unix网络编程 之 ourhdr.h文件</title>
      <link>https://realjf.io/files/ourhdr-h/</link>
      <pubDate>Sat, 23 Nov 2019 08:51:24 +0800</pubDate>
      
      <guid>https://realjf.io/files/ourhdr-h/</guid>
      <description>unix网络编程 之 ourhdr.h文件 ourhdr.h
// ourhdr.h #ifndef __ourhdr_h #define __ourhdr_h #include &amp;lt;errno.h&amp;gt; /*for definition of errno */ #include &amp;lt;stdarg.h&amp;gt; /*ANSI C header file */ #include &amp;lt;sys/types.h&amp;gt; /* required for some of our prototypes */ #include &amp;lt;stdio.h&amp;gt; /* for convenience */ #include &amp;lt;stdlib.h&amp;gt; /* for convenience */ #include &amp;lt;string.h&amp;gt; /* for convenience */ #include &amp;lt;unistd.h&amp;gt; /* for convenience */ #define MAXLINE 4096 /* max line length */ #define FILE_MODE (S_IRUSR | S_IWUSR | S_IRGRP | S_IROTH) /* default file access permissions for new files */ #define DIR_MODE (FILE_MODE | S_IXUSR | S_IXGRP | S_IXOTH) /* default permissions for new directoris */ typedef void Sigfunc(int); /* for signal handlers */ /* 4.</description>
    </item>
    
    <item>
      <title>unix网络编程 之 unp.h头文件安装配置</title>
      <link>https://realjf.io/unix/how-to-setup-unp-env/</link>
      <pubDate>Sat, 23 Nov 2019 08:17:24 +0800</pubDate>
      
      <guid>https://realjf.io/unix/how-to-setup-unp-env/</guid>
      <description>unix网络编程环境之unp.h安装配置 官网http://www.unpbook.com/src.html
1. 下载安装 wget http://www.unpbook.com/unpv13e.tar.gz tar zxvf unpv13e.tar.gz cd unpv13e ./configure cd lib make cd ../libfree make  上面make遇到报错，如下
gcc -I../lib -g -O2 -D_REENTRANT -Wall -c -o in_cksum.o in_cksum.c gcc -I../lib -g -O2 -D_REENTRANT -Wall -c -o inet_ntop.o inet_ntop.c inet_ntop.c: In function ‘inet_ntop’: inet_ntop.c:60:9: error: argument ‘size’ doesn’t match prototype size_t size; ^~~~ In file included from inet_ntop.c:27: /usr/include/arpa/inet.h:64:20: error: prototype declaration extern const char *inet_ntop (int __af, const void *__restrict __cp, ^~~~~~~~~ make: *** [&amp;lt;builtin&amp;gt;: inet_ntop.</description>
    </item>
    
    <item>
      <title>Golang 并发编程 之 sync.Mutex 或 channel（通道）</title>
      <link>https://realjf.io/posts/golang-concurrency-mutexorchannel/</link>
      <pubDate>Thu, 21 Nov 2019 18:01:02 +0800</pubDate>
      
      <guid>https://realjf.io/posts/golang-concurrency-mutexorchannel/</guid>
      <description>并发控制中sync.Mutex 与 channel 的使用？ go的创建者建议“通过通信共享内存，不通过共享内存进行通信”。
也就是说，Go确实在sync包中提供了传统的锁定机制。大多数锁定问题可以使用通道锁定或传统锁定来解决
使用锁机制和通道的优劣分析 Go新手常见的错误是仅仅因为可能和/或很有趣而过度使用通道和goroutine。如果最适合您的问题，请不要害怕使用sync.Mutex。 Go务实的做法是让您使用能够最好地解决问题的工具，而不用强迫您使用一种代码风格.
通常
   channel mutex     相互传递数据，分发工作单元，传递异步结果 缓存，状态    wait-group 另一个重要的同步机制是sync.WaitGroup。 这允许多个协作goroutine在再次独立运行之前共同等待同一个阈值事件。
通常在两种情况下很有用。
 在“清理”时，可以使用sync.WaitGroup来确保所有goroutine（包括主要的goroutine）都在完全终止之前等待 更常见的情况是循环算法，其中涉及一组goroutine，这些goroutine全部独立工作一段时间，然后全部等待障碍，然后再次独立进行。此模式可能会重复很多次。障碍事件可能会交换数据。此策略是批量同步并行（BSP）的基础  结语 怎么使用取决于你的应用场景，通道通信，互斥锁和等待组是互补的，可以组合使用。</description>
    </item>
    
    <item>
      <title>Golang 并发编程 之 runtime.LockOSThread</title>
      <link>https://realjf.io/posts/golang-concurrency-lockosthread/</link>
      <pubDate>Thu, 21 Nov 2019 17:10:37 +0800</pubDate>
      
      <guid>https://realjf.io/posts/golang-concurrency-lockosthread/</guid>
      <description>背景介绍 一些库（尤其是图形框架和库（例如Cocoa，OpenGL和libSDL））使用线程局部状态，并且可能要求仅从特定OS线程（通常是“主”线程）调用函数。 Go为此提供了runtime.LockOSThread函数，接下来通过示例说明如何正确使用它。
package dl import ( &amp;quot;fmt&amp;quot; &amp;quot;runtime&amp;quot; ) // 安排main.main在主线程上运行 func init() { runtime.LockOSThread() } // 在主线程main.main中调用Main循环 func Main() { for f := range mainfunc { // 取出工作队列中的函数进行调用 f() } } var mainfunc = make(chan func()) func do(f func()) { done := make(chan bool, 1) // 将整个函数加入到工作队列中 mainfunc &amp;lt;- func() { f() fmt.Println(&amp;quot;add queue&amp;quot;) done &amp;lt;- true } &amp;lt;-done } func Beep() { do(func() { // 无论什么时候都运行在主线程 fmt.</description>
    </item>
    
    <item>
      <title>Golang 并发编程 之 超时处理</title>
      <link>https://realjf.io/posts/golang-concurrency-timeout/</link>
      <pubDate>Thu, 21 Nov 2019 17:10:13 +0800</pubDate>
      
      <guid>https://realjf.io/posts/golang-concurrency-timeout/</guid>
      <description>并发编程中的超时处理 在并发编程中，要放弃运行时间太长的同步调用，请使用带有time.After的select语句，如下：
import ( &amp;quot;errors&amp;quot; &amp;quot;fmt&amp;quot; &amp;quot;time&amp;quot; ) func main() { var timeoutNanoseconds time.Duration = 5 * time.Second c := make(chan error, 1) go func() { time.Sleep(20 * time.Second) c &amp;lt;- errors.New(&amp;quot;error&amp;quot;) } () select { case err := &amp;lt;-c: // use err and reply fmt.Println(err) case &amp;lt;-time.After(timeoutNanoseconds): // call timed out fmt.Println(&amp;quot;timeout...&amp;quot;) } }  以上代码在超时5秒后退出</description>
    </item>
    
    <item>
      <title>Golang 并发编程 之 数据竞态检测</title>
      <link>https://realjf.io/posts/golang-data-race-detector/</link>
      <pubDate>Thu, 21 Nov 2019 16:41:31 +0800</pubDate>
      
      <guid>https://realjf.io/posts/golang-data-race-detector/</guid>
      <description>什么是数据争用或竞态 数据争用是并发系统中最常见且最难调试的错误类型之一。当两个goroutine并发访问同一变量并且至少其中之一是写操作时，就会发生数据争用。
下面让我们来实际模拟一下数据争用问题。
以下示例可能导致内存崩溃和损坏的数据争用
func main() { c := make(chan bool) m := make(map[string]string) go func() { m[&amp;quot;1&amp;quot;] = &amp;quot;a&amp;quot; c &amp;lt;- true }() m[&amp;quot;2&amp;quot;] = &amp;quot;b&amp;quot; &amp;lt;-c for k, v := range m { fmt.Println(k, v) } }  运行go run -race main.go进行竞争检测，得到的结果如下：
#================== WARNING: DATA RACE Write at 0x00c00008e150 by goroutine 6: runtime.mapassign_faststr() /usr/local/go/src/runtime/map_faststr.go:202 +0x0 main.main.func1() /root/go_project/src/test/race.go:9 +0x5d Previous write at 0x00c00008e150 by main goroutine: runtime.mapassign_faststr() /usr/local/go/src/runtime/map_faststr.go:202 +0x0 main.</description>
    </item>
    
    <item>
      <title>Protobuf  数据类型</title>
      <link>https://realjf.io/posts/protobuf-data-type/</link>
      <pubDate>Mon, 28 Oct 2019 15:31:06 +0800</pubDate>
      
      <guid>https://realjf.io/posts/protobuf-data-type/</guid>
      <description>基础类型    .proto类型 java类型 c++类型 备注     double double double    float float float    int32 int int32 使用可变长编码方式。编码负数时不够高效，如果你的字段可能包含负数，请使用sint32   int64 long int64 使用可变长编码方式。编码负数时不够高效，如果你的字段可能包含负数，请使用sint64   uint32 int[1] uint32 总是4个字节，如果数值总是比228大的话，这个类型会比uint32高效   uint64 long[1] uint64 总是8个字节，如果数值总是比256大的话，这个类型会比uint64高效   sint32 int int32 使用可变编码方式，有符号的整型值，编码时比通常的int32高效   sint64 long int64 使用可变长编码方式，有符号的整型值，编码时比通常的int64高效   fixed32 int[1] uint32 总是4个字节。如果数值总是比总是比228大的话，这个类型会比uint32高效。   fixed64 long[1] unit64 总是8个字节。如果数值总是比256大的话，这个类型会比uint64高效   sfixed32 int int32 总是4个字节   sfixed64 long int64 总是8个字节   bool boolean bool    string String string 一个字符串必须是utf-8编码或者7-bit ascii编码的文本   bytes ByteString string 可能包含任意顺序的字节数据    特殊字段    英文 中文 备注     enum 枚举(数字从零开始) 作用是为字段指定某”预定义值序列” enum Type {MAN = 0;WOMAN = 1; OTHER= 3;}   message 消息体 message User{}   repeated 数组/集合 repeated User users = 1   import 导入定义 import &amp;ldquo;protos/other_protos.</description>
    </item>
    
    <item>
      <title>Golang Micro 微服务框架使用</title>
      <link>https://realjf.io/posts/golang-micro-usage/</link>
      <pubDate>Tue, 22 Oct 2019 09:50:59 +0800</pubDate>
      
      <guid>https://realjf.io/posts/golang-micro-usage/</guid>
      <description>准备  搭建好golang开发环境 安装git等相关工具  开始 一、安装protobuf protobuf用于生成微服务代码
go get github.com/micro/protoc-gen-micro # 同时需要安装protoc和protoc-go-gen go get -d -u github.com/golang/protobuf/protoc-gen-go go install github.com/golang/protobuf/protoc-gen-go   如果需要别的语言的代码生成器，请参阅https://github.com/protocolbuffers/protobuf
关于protobuf的使用，请参阅https://developers.google.com/protocol-buffers/
 二、服务发现 服务发现用于将服务名称解析为地址，服务发现可以使用etcd、zookeeper、consul等组件
安装etcd etcd下载地址https://github.com/etcd-io/etcd/releases
三、写一个服务 以下为一个简单的rpc服务例子
创建服务proto 微服务的关键要求之一是严格定义接口。
Micro使用protobuf来实现这一目标。 在这里，我们使用Hello方法定义了Greeter处理程序。 它需要一个字符串参数同时使用一个HelloRequest和HelloResponse。
syntax = &amp;quot;proto3&amp;quot;; service Greeter { rpc Hello(HelloRequest) returns (HelloResponse) {} } message HelloRequest { string name = 1; } message HelloResponse { string greeting = 2; }  生成proto protoc --proto_path=$GOPATH/src:. --micro_out=.</description>
    </item>
    
    <item>
      <title>Golang语言标准库之 sync/atomic原子操作</title>
      <link>https://realjf.io/posts/sync-atomic/</link>
      <pubDate>Thu, 17 Oct 2019 17:37:02 +0800</pubDate>
      
      <guid>https://realjf.io/posts/sync-atomic/</guid>
      <description>原子操作，顾名思义是不可分割的，他可以是一个步骤，也可以是多个步骤，其执行过程不会被线程调度机制打断的操作。
 原子性不可能由软件单独保证，需要硬件的支持，因此和架构有关。在x86架构平台下，cpu提供了在指令执行期间对总线加锁的手段。
CPU芯片上有一条引线#HLOCK pin，如果汇编语言的程序中在一条指令前面加上前缀&amp;rdquo;LOCK&amp;rdquo;，经过汇编以后的机器代码就使CPU在执行这条指令的时候把#HLOCK pin的电位拉低，
持续到这条指令结束时放开，从而把总线锁住，这样同一总线上别的CPU就暂时不能通过总线访问内存了，保证了这条指令在多处理器环境中的原子性。
 sync/atomic包的文件结构以及数据结构可以参考这里
sync/atomic包提供了6中操作数据类型
 int32 uint32 int64 uint64 uintptr unsafe.Pointer  分别为这每种数据类型提供了五种操作
 add 增减 load 载入 store 存储 compareandswap 比较并交换 swap 交换  下面以int32为例，具体使用上面五种操作实现原子操作 AddInt32操作 var val int32 val = 10 atomic.AddInt32(&amp;amp;val, 10) // 对于无符号32位即uint32，则需要使用二进制补码进行操作 var val2 uint32 val2 = 10 atomic.AddUint32(&amp;amp;val2, ^uint32(10 - 1)) // 等价于 val2 - 10  CompareAndSwapInt32 对比并交换是指先判断addr指向的值是否与参数old一致，如果一致就用new值替换addr的值，最后返回成功，具体例子如下
package main import ( &amp;quot;fmt&amp;quot; &amp;quot;sync&amp;quot; &amp;quot;sync/atomic&amp;quot; ) func main() { var val int32 wg := sync.</description>
    </item>
    
    <item>
      <title>Mysql 5.7.27源码安装教程</title>
      <link>https://realjf.io/posts/mysql-5.7-installation/</link>
      <pubDate>Tue, 15 Oct 2019 09:11:41 +0800</pubDate>
      
      <guid>https://realjf.io/posts/mysql-5.7-installation/</guid>
      <description>准备  debian 9操作系统 mysql下载地址：https://downloads.mysql.com/archives/get/file/mysql-5.7.27.tar.gz boost下载地址：http://nchc.dl.sourceforge.net/project/boost/boost/1.59.0/boost_1_59_0.tar.gz  下载安装 1. 下载安装boost wget http://nchc.dl.sourceforge.net/project/boost/boost/1.59.0/boost_1_59_0.tar.gz tar zxvf boost_1_59_0.tar.gz mv boost_1_59_0 /usr/local/boost  2. 下载安装mysql # 安装依赖包 apt-get install libncurses-dev # 创建mysql用户组和用户 groupadd mysql useradd mysql -s /sbin/nologin -M -g mysql # 下载mysql wget https://downloads.mysql.com/archives/get/file/mysql-5.7.27.tar.gz tar zxvf mysql-5.7.27.tar.gz cd mysql-5.7.27 # 创建必要的文件夹 mkdir /usr/local/mysql mkdir /usr/local/mysql/data # 数据库文件 mkdir /usr/local/mysql/tmp # sock文件 mkdir /usr/local/mysql/logs # 错误日志文件 mkdir /usr/local/mysql/binlog # binlog日志文件 # 编译mysql cmake .</description>
    </item>
    
    <item>
      <title>Mysql Community Server Installation(mysql 8.0.17 社区版本安装教程)</title>
      <link>https://realjf.io/posts/mysql-community-server-installation/</link>
      <pubDate>Mon, 14 Oct 2019 17:50:16 +0800</pubDate>
      
      <guid>https://realjf.io/posts/mysql-community-server-installation/</guid>
      <description>一、下载安装 下载地址：https://downloads.mysql.com/archives/community/
# 下载 wget https://downloads.mysql.com/archives/get/file/mysql-8.0.17-linux-glibc2.12-x86_64.tar.xz xz -d mysql-8.0.17-linux-glibc2.12-x86_64.tar.xz tar xvf mysql-8.0.17-linux-glibc2.12-x86_64.tar # 移动到你需要安装的目录下 mv mysql-8.0.17-linux-glibc2.12-x86_64 /usr/local/mysql  二、配置 1. 在mysql根目录下创建一个新的data目录，用于存放数据 cd /usr/local/mysql mkdir data  2. 创建mysql用户组和mysql用户 groupadd mysql useradd -g mysql mysql  3. 改变mysql目录权限 chown -R mysql.mysql /usr/local/mysql/  4. 初始化数据库 # 创建mysql_install_db安装文件 mkdir mysql_install_db chmod 777 ./mysql_install_db # 初始化数据库 bin/mysqld --initialize --user=mysql --basedir=/usr/local/mysql --datadir=/usr/local/mysql/data # 记录好自己的临时密码  5. mysql配置 cp /usr/local/mysql/support-files/mysql.server /etc/init.d/mysqld  修改my.cnf文件
vim /etc/my.</description>
    </item>
    
    <item>
      <title>Django后端 &#43; Vue前端 构建Web开发框架</title>
      <link>https://realjf.io/posts/django-vue-web/</link>
      <pubDate>Mon, 14 Oct 2019 15:17:48 +0800</pubDate>
      
      <guid>https://realjf.io/posts/django-vue-web/</guid>
      <description>一、准备  Django &amp;gt;= 1.11 python &amp;gt;= 3.6 mysql &amp;gt;= 5.7 node &amp;gt;= 10.15 vue-cli &amp;gt;= 2.0   本次实验项目基于debian 9系统进行构建，以下涉及到的一些安装命令请根据自己具体环境自行替换
 二、安装 1. 安装node wget https://nodejs.org/dist/v10.16.3/node-v10.16.3-linux-x64.tar.xz xz -d node-v10.16.3-linux-x64.tar.xz tar xvf node-v10.16.3-linux-x64.tar # 然后将文件夹移动到你需要的地方，设置环境变量PATH即可 mv node-v10.16.3-linux-x64 /usr/local/node-v10.16.3 # 这里使用软链进行设置l ln -sf /usr/local/node-v10.16.3/bin/node /usr/local/bin/ ln -sf /usr/local/node-v10.16.3/bin/npm /usr/local/bin/ # 设置好后进行测试 node --version npm --version  2. 安装python3，pip # 打开下载地址 https://www.python.org/downloads/source/ # 选择适合自己的包下载 wget https://www.python.org/ftp/python/3.7.4/Python-3.7.4.tar.xz xz -d Python-3.7.4.tar.xz tar xvf Python-3.</description>
    </item>
    
    <item>
      <title>Makefile 基本语法和规则</title>
      <link>https://realjf.io/devtools/makefile-rule/</link>
      <pubDate>Mon, 30 Sep 2019 21:57:15 +0800</pubDate>
      
      <guid>https://realjf.io/devtools/makefile-rule/</guid>
      <description> 基本语法 target1 target2 target3: prerequisite1 prerequisite2 command1 command2 command3  冒号的左边可以出现一个或多个工作目标，而冒号的右边可以出现零个或多个必要条件。 如果冒号的右边没有指定必要条件，那么只有在工作目标所代表的文件不存在时才会进行更新的动作。
每个命令必须以跳格符开头，这个语法用来要求make将紧跟在跳格符之后的内容传给subshell来执行。
make会将#号视为注释字符，从井号开始到该行结束之间的所有文字都会被make忽略。你可以使用反斜线，来延续过长的文本行。
规则 </description>
    </item>
    
    <item>
      <title>Nginx服务的基本配置</title>
      <link>https://realjf.io/posts/nginx-base-setting/</link>
      <pubDate>Mon, 30 Sep 2019 14:56:20 +0800</pubDate>
      
      <guid>https://realjf.io/posts/nginx-base-setting/</guid>
      <description>按照用户使用时的预期功能分成了4个功能
 用于调试、定位问题的配置项 正常运行的必备配置项 优化性能的配置项 事件类配置项  用于调试进程和定位问题的配置项 1. 是否以守护进程方式运行nginx 语法： daemon on|off;
默认：daemon on;
守护进程是脱离终端并且在后台运行的进程。它脱离终端是为了避免进程执行过程中的信息在任何终端中显示，这样一来，进程也不会被任何终端所产生的信息所打断。 因此，默认都是以这种方式运行的。
2. 是否以master/worker方式运行 语法： master_process on|off;
默认： master_process on;
一个master进程管理多个worker进程的方式运行的，几乎所有的产品环境下，nginx都是以这种方式工作。
3. error日志的配置 语法：error_log /path/file level;
默认：error_log logs/error.log error;
error日志是定位nginx问题的最佳工具，我们可以根据自己的需求妥善设置error日志的路径和级别。
/path/file参数可以是一个具体的文件，最好将它放到一个磁盘足够大的位置； 也可以是/dev/null，这样就不会输出任何日志了，这也是关闭error日志的唯一手段； 也可以是stderr，这样日志会输出到标准错误文件中。
level是日志的输出级别，取值范围是debug、info、notice、warn、error、crit、alert、emerg。 当设置一个级别，大于或等于该级别的日志都会被输出到/path/file文件中。小鱼该级别的日志则不会输出。
4. 是否处理几个特殊的调试点 语法：debug_points [stop|abort]
这个配置项也是用来帮助用户跟踪调试nginx的。他接受两个参数：stop和abort。 nginx在一些关键的错误逻辑中设置了调试点。如果设置了debug_points为stop，那么nginx的代码执行到这些调试点时就会发出sigstop信号用以调试。 如果设置为abort，则会生成一个coredump文件，可以使用gdb来查看nginx当时的各种信息。
通常不会使用这个配置项。
5. 仅对指定的客户端输出debug级别的日志 语法：debug_connection [IP|CIDR]
这个配置项实际上属于事件类配置，因此，他必须放在events{&amp;hellip;}中才有效，他的值可以是ip地址或cidr地址，如：
events{ debug_connection 10.224.66.14; debug_connection 10.224.57.0/24; }  这样，仅仅来自以上ip地址的请求才会输出debug级别的日志，其他请求仍然沿用error_log中配置的日志级别。
这个配置对修复bug很有用，特别是定位高并发请求下才会发生的问题。
 在debug_connection前，需要确保在执行configure时已经加入了&amp;ndash;with-debug参数，否则不会生效。
 6. 限制coredump核心转储文件的大小 语法：worker_rlimit_core size;</description>
    </item>
    
    <item>
      <title>Linux 内核参数优化</title>
      <link>https://realjf.io/posts/linux-kernel-optimize/</link>
      <pubDate>Mon, 30 Sep 2019 13:50:42 +0800</pubDate>
      
      <guid>https://realjf.io/posts/linux-kernel-optimize/</guid>
      <description>由于默认的linux内核参数考虑的是最通用的场景，这种场景下并不适合高并发访问的web服务器的定义，所以需要修改如下参数， 使得nginx可以拥有更高的性能。
根据不同的业务特点，nginx作为静态web内容服务器、反向代理服务器或者提供图片缩略图功能（实时亚索图片）的服务器时， 其内核参数调整是不同的。
这里只针对最通用，使nginx支持更多并发请求的tcp网络参数做简单说明。
需要修改/etc/sysctl.conf来更改内核参数。
fs.file-max = 999999 net.ipv4.tcp_tw_reuse = 1 net.ipv4.tcp_keepalive_time = 600 net.ipv4.tcp_fin_timeout = 30 net.ipv4.tcp_max_tw_buckets = 5000 net.ipv4.ip_local_port_range = 1024 61000 net.ipv4.tcp_rmem = 4096 32768 262142 net.ipv4.tcp_wmem = 4096 32768 262142 net.core.netdev_max_backlog = 8096 net.core.rmem_default = 262144 net.core.wmem_default = 262144 net.rmem_max = 2097152 net.wmem_max = 2097152 net.ipv4.tcp_syncookies = 1 net.ipv4.tcp_max_syn.backlog = 1024  参数说明
 file-max: 这个参数表示进程可以同时打开的最大句柄数，这个参数直接限制最大并发连接数，需要根据实际情况配置 tcp_tw_reuse: 这个参数设置为1，表示允许将TIME_WAIT状态的socket重新用于新的tcp连接，这对于服务器来说很有意义，因为服务器上总会有大量TIME-WAIT状态的连接 tcp_keepalive_time: 这个参数表示当keepalive启用时，tcp发送keepalive消息的频度。默认是2小时，若将其设置的小一些，可以更快地清理无效的连接 tcp_fin_timeout: 这个参数表示当服务器主动关闭连接时，socket保持在FIN-WAIT-2状态的最大时间。 tcp_max_tw_buckets: 这个参数表示操作系统允许TIME-WAIT套接字数量的最大值，如果超过这个数字，TIME-WAIT套接字将立刻被清除并打印警告信息。这个参数默认为180000，过多的TIME-WAIT套接字会使web服务器变慢。 tcp_max_syn_backlog: 这个参数表示TCP三次握手建立阶段接收syn请求队列的最大长度，默认为1024，将其设置得大一些可以使出现nginx繁忙来不及accept新连接的情况时，linux不至于丢失客户端发起的连接请求。 ip_local_port_range: 这个参数定义了在udp和tcp连接中本地（不包括连接的远端）端口的取值范围。 net.</description>
    </item>
    
    <item>
      <title>Scan Qrcode</title>
      <link>https://realjf.io/posts/scan-qrcode/</link>
      <pubDate>Wed, 04 Sep 2019 09:28:23 +0800</pubDate>
      
      <guid>https://realjf.io/posts/scan-qrcode/</guid>
      <description>网页调用微信JSSDK实现扫一扫功能 设置公众号js接口安全域 在公众号后台的，公众号设置，功能设置里
配置ip白名单 在公众号后台基本配置里
页面引入微信sdkjs代码 http://res.wx.qq.com/open/js/jweixin-1.2.0.js
页面js代码 // 点击扫一扫按钮事件 $(&amp;quot;#btn-scan&amp;quot;).on(&amp;quot;click&amp;quot;, function () { //微信扫一扫 设置 var _queryString = window.location.search; $.ajax({ type: &amp;quot;post&amp;quot;, url: &amp;quot;/mobile/user/scanSign&amp;quot;, data: {query: _queryString}, success: function (data) { var result = data.result; wx.config({ debug: false, // 调试接口用 appId: result.appId, //公众号的唯一标识 timestamp: &amp;quot;&amp;quot; + result.timestamp, //生成签名的时间戳 nonceStr: result.nonceStr, //生成签名的随机串 signature: result.signature, //签名 jsApiList: [&#39;scanQRCode&#39;] //需要使用的JS接口列表(我只需要调用扫一扫的接口，如有多个接口用逗号分隔) }); } }); }); //微信扫一扫处理代码 wx.ready(function () { $(&amp;quot;body&amp;quot;).off(&amp;quot;click&amp;quot;, &amp;quot;.j-btn_chat&amp;quot;).on(&amp;quot;click&amp;quot;, &amp;quot;.j-btn_chat&amp;quot;, function (e) { wx.</description>
    </item>
    
    <item>
      <title>Ruby 环境安装</title>
      <link>https://realjf.io/posts/ruby-installation/</link>
      <pubDate>Fri, 30 Aug 2019 12:42:08 +0800</pubDate>
      
      <guid>https://realjf.io/posts/ruby-installation/</guid>
      <description> centos7 下进行安装ruby 准备 下载ruby ruby下载地址：http://www.ruby-lang.org/en/downloads/
这里以2.6.4版本为例
wget https://cache.ruby-lang.org/pub/ruby/2.6/ruby-2.6.4.tar.gz  解压配置安装 tar zxvf ruby-2.6.4.tar.gz -C /usr/local/ cd /usr/local/ruby-2.6.4/ ./configure make &amp;amp;&amp;amp; make install  添加到环境变量中 ln -s /usr/local/ruby-2.6.4/ruby /usr/bin/ruby  验证 ruby -v  </description>
    </item>
    
    <item>
      <title>Srs Obs FFmpeg Vlc搭建rtmp直播服务，并实现推流拉流</title>
      <link>https://realjf.io/posts/srs-obs-ffmpeg-vlc/</link>
      <pubDate>Wed, 10 Jul 2019 16:06:30 +0800</pubDate>
      
      <guid>https://realjf.io/posts/srs-obs-ffmpeg-vlc/</guid>
      <description>rtmp srs直播服务器搭建 准备  srs 提供直播流服务器 obs 提供推流服务 ffmpeg 强大的软件，可作为推流端使用 vlc 用于播放rtmp直播  1. 首先搭建rtmp srs服务器 git clone https://github.com/ossrs/srs cd srs/trunk # 构建srs ./configure &amp;amp;&amp;amp; make # 开启服务 ./objs/srs -c conf/srs.conf # 停止服务 ./objs/srs stop # 重启服务 ./objs/srs restart  2. 安装obs apt-get install obs-studio  关于obs推流设置https://obsproject.com/wiki/OBS-Studio-Quickstart
3. 安装vlc apt-get install vlc  在推流设置完成后，测试推流效果步骤如下： 1. 打开VLC，选择open media-&amp;gt;network 2. 在网络协议中输入推流地址，点击play即可
4. 安装ffmpeg git clone https://git.ffmpeg.org/ffmpeg.git ffmpeg cd ffmpeg # 编译ffmpeg .</description>
    </item>
    
    <item>
      <title>Sublimetext debian安装与常用插件配置</title>
      <link>https://realjf.io/devtools/sublimetext/</link>
      <pubDate>Fri, 05 Jul 2019 10:27:14 +0800</pubDate>
      
      <guid>https://realjf.io/devtools/sublimetext/</guid>
      <description>sublime text官网http://www.sublimetext.com
 安装 install the GPG key wget -qO - https://download.sublimetext.com/sublimehq-pub.gpg | sudo apt-key add -  确保apt工作在http源 apt-get install apt-transport-https  选择安装渠道 稳定版本
echo &amp;quot;deb https://download.sublimetext.com/ apt/stable/&amp;quot; | sudo tee /etc/apt/sources.list.d/sublime-text.list  开发版本
echo &amp;quot;deb https://download.sublimetext.com/ apt/dev/&amp;quot; | sudo tee /etc/apt/sources.list.d/sublime-text.list  更新源并安装 apt-get update apt-get install sublime-text  安装常用插件 1. 安装Package Control 请参考网址Install Package Control
2. 常用插件 ConvertToUTF8 功能：能将除UTF8编码之外的其他编码文件在 Sublime Text 中转换成UTF8编码，在打开文件的时候一开始会显示乱码，然后一刹那就自动显示出正常的字体，当然，在保存文件之后原文件的编码格式不会改变
BracketHighlighter 功能：高亮显示匹配的括号、引号和标签。
Emmet 功能：前端开发必备，HTML、CSS代码快速编写神器</description>
    </item>
    
    <item>
      <title>分布式系统 之 容错性</title>
      <link>https://realjf.io/posts/fault-tolerance/</link>
      <pubDate>Thu, 28 Mar 2019 21:44:20 +0800</pubDate>
      
      <guid>https://realjf.io/posts/fault-tolerance/</guid>
      <description>容错性 基本概念 容错与系统可靠性息息相关，可靠系统满足以下特性：
 可用性 可靠性 安全性 可维护性  故障分类 故障通常分为三类
 暂时故障 间歇故障 持久故障  分布式系统中的典型故障模式可分为以下几种：
 崩溃性故障 遗漏性故障 定时性故障 响应性故障 任意性故障  任意性故障是最严重的故障，也称拜占庭故障。
分布式提交 在分布式系统中，事务往往包含多个参与者的活动，单个参与者的活动是能够保证原子性的， 而保证多个参与者之间原子性则需要通过两阶段提交或者三阶段提交算法实现。
两阶段提交 两阶段提交协议（2PC）的过程涉及协调者和参与者。协调者可以看做事务的发起者，同时也是事务的一个参与者。 对于一个分布式事务来说，一个事务是涉及多个参与者的。
第一阶段(准备阶段)
 协调者节点向所有参与者节点询问是否可以执行提交操作，并开始等待各参与者节点的响应。 参与者节点执行所有事务操作，并将undo信息和redo信息写入日志（若成功其实这里每个参与者已经执行了事务操作） 个参与者节点响应协调者节点发起的询问。如果参与者节点的事务操作实际执行成功，则它返回一个同意消息，如果参与者节点事务操作实际执行失败，则返回一个终止操作  第二阶段（提交阶段）
如果协调者收到了参与这的失败消息或者超时，直接给每个参与者发送回滚消息，否则，发送提交消息； 参与者根据协调者的指令执行提交或者回滚操作，释放所有事务处理过程中使用的锁资源。
 当协调者节点从所有参与者节点处获得的相应消息都为同意时：  协调者节点向所有参与者节点发送正式提交请求 参与者节点正式完成操作，并释放在整个事务期间内占用的资源 参与者节点向协调者节点发送完成消息  如果任一参与者节点在第一阶段返回的消息为终止，或者协调者节点在第一阶段的询问在超时之前无法获取所有参与者节点的响应消息时：  协调者节点向所有参与者节点发送回滚操作请求 参与者节点利用之前写入的undo信息执行回滚，并释放在整个事务期间内占用的资源 参与者节点向协调者节点发送回滚完成消息 协调者节点收到所有参与者节点反馈的回滚完成消息后，取消事务 协调者节点收到所有参与者节点返回的完成消息后，完成事务。    缺点
 同步阻塞问题。执行过程中，所有参与者节点都是事务阻塞型的。 单点故障问题。由于协调者的重要性，一旦协调者发生故障，参与者会一直阻塞下去。 数据不一致。在阶段二中，当协调者向参与者发送commit请求后，发生了局域网异常，或者在发送commit请求过程中协调者发生故障， 这会导致只有一部分参与者接收到了commit请求。而在这部分参与者接收到commit请求之后就会执行commit操作。但是其他部分未接收到commit请求的机器无法执行事务提交。于是整个分布式系统便出现了数据不一致性的现象。 两阶段无法解决的问题：协调者再发出commit消息之后宕机，而唯一接收到这条消息的参与者同时也宕机了，那么， 即使协调者通过选举产生了新的协调者，这条事务的状态也是不确定的，没人知道事务是否已被提交。  为了解决两阶段提交的种种问题，提出了三阶段提交。
三阶段提交 三阶段提交是两阶段提交的改进版，有 两个改动点：</description>
    </item>
    
    <item>
      <title>linux Cgroups</title>
      <link>https://realjf.io/posts/cgroups/</link>
      <pubDate>Thu, 21 Mar 2019 05:14:39 +0800</pubDate>
      
      <guid>https://realjf.io/posts/cgroups/</guid>
      <description>Namespace技术为docker容器做了重要的隔离，但是docker容器每个隔离空间之间怎么保持独立而不互相竞争资源呢？这就是cgroups要做的事情了
Linux Cgroups(control groups)提供了对一组进程及其子进程的资源限制、控制和统计的能力，包括cpu、内存、存储和网络等。
cgroups组件  cgroup subsystem hierarchy  cgroup cgroup是对进程分组管理的一种机制，一个cgroup包含一组进程，并可以在这个cgroup上增加linux subsystem的各种配置参数，将一组进程和一组subsystem的系统参数关联起来。
subsystem 是一组资源控制的模块，包括 - blkio 设置对块设备输入输出的访问控制 - cpu 设置cgroup 中进程的cpu被调度的策略 - cpuacct 可以统计cgroup中进程的cpu占用 - cpuset 在多核机器上设置cgroup中进程可以使用的cpu和内存 - devices 控制cgroup中进程对设备的访问 - freezer 用于挂起和恢复cgroup中的进程 - memory 用于控制cgroup中进程的内存占用 - net_cls 用于将cgroup中进程产生的网络包分类，以便linux的tc可以根据分类区分来自某个cgroup的包并做限流和监控 - ns 使cgroup中的进程在新的namespace中fork新进程时，创建出一个新的cgroup，这个cgroup包含新的namespace中的进程
每个subsystem会关联到定义了相应限制的cgroup上，并对这个cgroup中的进行做相应的限制和控制。这些subsystem是逐步合并到内核中的。
 如何看内核当前支持哪些subsystem呢？使用apt-get install cgroup-bin，然后通过lssubsys -a查看
 hierarchy 把一组cgroup串成一个树状结构，一个这样的树便是一个hierarchy，通过这种树状结构，cgroups可以形成继承关系。
三个组件的关系  系统在创建了新的hierarchy之后，系统中所有的进程都会加入这个hierarchy的cgroup根节点，这个cgroup根节点是hierarchy默认创建的 一个subsystem只能附加到一个hierarchy上面 一个hierarchy可以附加多个subsystem 一个进程可以作为多个cgroup的成员，但是这些cgroup必须在不同的hierarchy中。 一个进程fork出子进程时，子进程是和父进程在同一个cgroup中的，也可以根据需要将其移动到其他cgroup中。  kernel加载Cgroups kernel通过虚拟树状文件系统配置cgroups，通过层级的目录虚拟出cgroup树。
1. 首先，要创建并挂载一个hierarchy mkdir cgroup-test mount -t cgroup -o none,name=cgroup-test cgroup-test .</description>
    </item>
    
    <item>
      <title>golang性能分析利器之Pprof</title>
      <link>https://realjf.io/posts/pprof/</link>
      <pubDate>Tue, 19 Mar 2019 15:14:16 +0800</pubDate>
      
      <guid>https://realjf.io/posts/pprof/</guid>
      <description>简介 pprof是golang程序一个性能分析的工具，可以查看堆栈、cpu信息等
pprof有2个包：net/http/pprof以及runtime/pprof
二者之间的关系：net/http/pprof包只是使用runtime/pprof包来进行封装了一下，并在http端口上暴露出来
性能分析利器 pprof go本身提供的工具链有： - runtime/pprof：采集程序的运行数据进行分析 - net/http/pprof：采集HTTP Server的运行时数据进行分析
pprof以profile.proto读取分析样本的集合，并生成报告以可视化并帮助分析数据
 profile.proto是一个Protocol Buffer v3的描述文件，它描述了一组callstack和symbolization信息，作用是表示统计分析的一组采样的调用栈，是很常见的stacktrace配置文件格式
 使用方式  Report generation：报告生成 Interactive terminal use：交互式终端使用 Web interface：Web界面  1. web服务器方式 假如你的go呈现的是用http包启动的web服务器，当想要看web服务器的状态时，选择【net/http/pprof】，使用方法如下：
&amp;quot;net/http&amp;quot; _ &amp;quot;net/http/pprof&amp;quot;  查看结果：通过访问：http://domain:port/debug/pprof查看当前web服务的状态
2. 服务进程 如果你go程序是一个服务进程，同样可以选择【net/http/pprof】包，然后开启另外一个goroutine来开启端口监听
// 远程获取pprof数据 go func() { log.Println(http.ListenAndServe(&amp;quot;localhost:8080&amp;quot;, nil)) }  3. 应用程序 如果你的go程序只是一个应用程序，那就直接使用runtime/pprof包，具体用法是用pprof.StartCPUProfile和pprof.StopCPUProfile。
var cpuprofile = flag.String(&amp;quot;cpuprofile&amp;quot;, &amp;quot;&amp;quot;, &amp;quot;write cpu profile to file&amp;quot;) func main() { flag.Parse() if *cpuprofile != &amp;quot;&amp;quot; { f, err := os.</description>
    </item>
    
    <item>
      <title>Ceph 集群搭建一 之 集群搭建</title>
      <link>https://realjf.io/posts/setup-ceph-cluster-3/</link>
      <pubDate>Tue, 19 Mar 2019 14:57:12 +0800</pubDate>
      
      <guid>https://realjf.io/posts/setup-ceph-cluster-3/</guid>
      <description>第一次练习时，我们创建一个 Ceph 存储集群，它有一个 Monitor 和两个 OSD 守护进程。一旦集群达到 active + clean 状态，再扩展它：增加第三个 OSD 、增加元数据服务器和两个 Ceph Monitors。为获得最佳体验，先在管理节点上创建一个目录，用于保存 ceph-deploy 生成的配置文件和密钥对。
 如果你是用另一普通用户登录的，不要用 sudo 或在 root 身份运行 ceph-deploy ，因为它不会在远程主机上调用所需的 sudo 命令。
 mkdir my-cluster cd my-cluster   禁用 requiretty 在某些发行版（如 CentOS ）上，执行 ceph-deploy 命令时，如果你的 Ceph 节点默认设置了 requiretty 那就会遇到报错。可以这样禁用此功能：执行 sudo visudo ，找到 Defaults requiretty 选项，把它改为 Defaults:ceph !requiretty ，这样 ceph-deploy 就能用 ceph 用户登录并使用 sudo 了。
 创建集群 如果在某些地方碰到麻烦，想从头再来，可以用下列命令配置：
ceph-deploy purgedata {ceph-node} [{ceph-node}] ceph-deploy forgetkeys  用下列命令可以连ceph安装包一起清除：</description>
    </item>
    
    <item>
      <title>Ceph 集群搭建二 之 预检</title>
      <link>https://realjf.io/posts/setup-ceph-cluster-2/</link>
      <pubDate>Tue, 19 Mar 2019 14:57:09 +0800</pubDate>
      
      <guid>https://realjf.io/posts/setup-ceph-cluster-2/</guid>
      <description>集群部署如下： 预检 安装ceph部署工具 在 Red Hat （rhel6、rhel7）、CentOS （el6、el7）和 Fedora 19-20 （f19 - f20） 上执行下列步骤：
用subscription-manager注册你的目标机器，确认你的订阅，并启用安装依赖包的extras软件仓库。例如： sudo subscription-manager repos --enable=el-7-server-extras-rpms  在centos上执行以下命令 sudo yum install -y yum-utils &amp;amp;&amp;amp; sudo yum-config-manager --add-repo https://dl.fedoraproject.org/pub/epel/7/x86_64/ &amp;amp;&amp;amp; sudo yum install --nogpgcheck -y epel-release &amp;amp;&amp;amp; sudo rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7 &amp;amp;&amp;amp; sudo rm /etc/yum.repos.d/dl.fedoraproject.org*  把软件包源加入软件仓库。用文本编辑器创建一个 YUM (Yellowdog Updater, Modified) 库文件，其路径为 /etc/yum.repos.d/ceph.repo sudo vim /etc/yum.repos.d/ceph.repo  把如下内容粘帖进去，用 Ceph 的最新主稳定版名字替换 {ceph-stable-release} （如 firefly，hammer, infernalis ），用你的Linux发行版名字替换 {distro} （如 el6 为 CentOS 6 、 el7 为 CentOS 7 、 rhel6 为 Red Hat 6.</description>
    </item>
    
    <item>
      <title>Ceph 集群搭建一 之 准备</title>
      <link>https://realjf.io/posts/setup-ceph-cluster-1/</link>
      <pubDate>Tue, 19 Mar 2019 14:57:05 +0800</pubDate>
      
      <guid>https://realjf.io/posts/setup-ceph-cluster-1/</guid>
      <description>1. 配置ceph yum源 vim /etc/yum.repos.d/ceph.repo [ceph-noarch] name=Cephnoarch packages baseurl=http://ceph.com/rpm-{ceph-release}/{distro}/noarch enabled=1 gpgcheck=1 type=rpm-md gpgkey=https://ceph.com/git/?p=ceph.git;a=blob_plain;f=keys/release.asc  ceph release http://docs.ceph.com/docs/master/releases/
2. 更新源并且安装hosts文件 yum update &amp;amp;&amp;amp; yum install ceph-deploy -y  3. 配置各节点hosts文件 cat /etc/hosts
192.168.1.2 node1 192.168.1.3 node2 192.168.1.4 node3  4. 配置各节点ssh无密码登录，通过ssh方式连接各节点服务器，以安装部署集群。输入ssh-keygen命令，在命令行输入以下内容： ssh-keygen  5. 拷贝key到各节点 ssh-copy-id node1 ssh-copy-id node2 ssh-copy-id node3  6. 在执行ceph-deploy的过程中会发生一些配置文件，建议创建一个目录 mkdir my-cluster cd my-cluster  7. 创建集群，部署新的monitor节点 ceph-deploy new {initial-monitor-node(s)} #例如 ceph-deploy new node1  8. 配置ceph.</description>
    </item>
    
    <item>
      <title>Golang GC 实现原理</title>
      <link>https://realjf.io/posts/how-golang-garbage-collection-works/</link>
      <pubDate>Tue, 19 Mar 2019 14:49:57 +0800</pubDate>
      
      <guid>https://realjf.io/posts/how-golang-garbage-collection-works/</guid>
      <description>当前的1.9版本的GC停顿时间已经可以做到极短. 停顿时间的减少意味着&amp;rdquo;最大响应时间&amp;rdquo;的缩短, 这也让go更适合编写网络服务程序. 这篇文章将通过分析golang的源代码来讲解go中的三色GC的实现原理.
基础概念 内存结构 go在程序启动时会分配一块虚拟内存地址是连续的内存，结构如下：
这一块内存分为了3个区域, 在X64上大小分别是512M, 16G和512G, 它们的作用如下:
arena arena区域就是我们通常说的heap, go从heap分配的内存都在这个区域中.
bitmap bitmap区域用于表示arena区域中哪些地址保存了对象, 并且对象中哪些地址包含了指针. bitmap区域中一个byte(8 bit)对应了arena区域中的四个指针大小的内存, 也就是2 bit对应一个指针大小的内存. 所以bitmap区域的大小是 512GB / 指针大小(8 byte) / 4 = 16GB.
bitmap区域中的一个byte对应arena区域的四个指针大小的内存的结构如下, 每一个指针大小的内存都会有两个bit分别表示是否应该继续扫描和是否包含指针:
bitmap中的byte和arena的对应关系从末尾开始, 也就是随着内存分配会向两边扩展:
spans spans区域用于表示arena区中的某一页(Page)属于哪个span, 什么是span将在下面介绍. spans区域中一个指针(8 byte)对应了arena区域中的一页(在go中一页=8KB). 所以spans的大小是 512GB / 页大小(8KB) * 指针大小(8 byte) = 512MB.
spans区域的一个指针对应arena区域的一页的结构如下, 和bitmap不一样的是对应关系会从开头开始:
什么时候从heap分配对象 go对自动确定哪些对象应该放在栈上，哪些对象应该放在堆上。 简单说，当一个对象的内容可能在生成该对象的函数结束后被访问，那么这个对象就会分配到堆上
在堆上分配的对象的情况包括： - 返回对象的指针 - 传递了对象的指针到其他函数 - 在闭包中是用来对象并且需要修改对象 - 使用new
在C语言中函数返回在栈上的对象的指针是非常危险的事情, 但在go中却是安全的, 因为这个对象会自动在堆上分配. go决定是否使用堆分配对象的过程也叫&amp;rdquo;逃逸分析&amp;rdquo;.
GC Bitmap GC在标记时需要知道哪些地方包含了指针, 例如上面提到的bitmap区域涵盖了arena区域中的指针信息.</description>
    </item>
    
    <item>
      <title>Goroutine 运行原理</title>
      <link>https://realjf.io/posts/goroutine-principle/</link>
      <pubDate>Tue, 19 Mar 2019 14:45:21 +0800</pubDate>
      
      <guid>https://realjf.io/posts/goroutine-principle/</guid>
      <description>Golang最大的特色可以说是协程(goroutine)了, 协程让本来很复杂的异步编程变得简单, 让程序员不再需要面对回调地狱, 虽然现在引入了协程的语言越来越多, 但go中的协程仍然是实现的是最彻底的.
核心概念 要理解协程的实现，需要理解三个重要概念，P、G和M。
G（goroutine） G是goroutine的简写，goroutine可以解释为受管理的轻量级线程，goroutine使用go关键字创建。
main函数是一个主线程，也是一个goroutine。
 goroutine的新建、休眠、回复、停止都受到go运行时的管理 goroutine执行异步操作时会进入休眠状态，待操作完成后在恢复，无需占用系统线程。 goroutine新建或恢复时会添加到运行队列，等待M取出并运行。  M（machine） M是machine的简写，表示系统线程
M可以运行两种代码： - go代码，即goroutine，M运行go代码需要一个P - 原生代码，例如阻塞的syscall，M运行原生代码不需要P
 M运行时，会从G可运行队列中取出一个然后运行，如果G运行完毕或者进入休眠状态，则从可运行队列中取下一个G运行，周而复始。 有时候G需要调用一些无法避免阻塞的原生代码，这时M会释放持有的P并进入阻塞状态。其他M会取得这个P并继续运行队列中的G。  go需要保证有足够的M可以运行G，不让CPU闲着，也需要保证M的数量不过多。
P（process） P是process的简写，代表M运行G所需要的资源。
 虽然P的数量默认等于cpu的核心数，但可以通过环境变量 GOMAXPROC 修改，在实际运行时P跟cpu核心并无任何关联。
 P也可以理解为控制go代码的并行度的机制 - 如果P的数量等于1，代表当前最多只能有一个线程M执行go代码。 - 如果P的数量等于2，代表当前最多只能有两个线程M执行go代码。
执行原生代码的线程数不受P控制。
因为同一时间只有一个线程M可以拥有P，P中的数据都是锁自由的，读写这些数据的效率会非常的高。
数据结构 G的状态  空闲中(_Gidle)：表示G刚刚新建，仍未初始化 待运行(_Grunnable)：表示G在运行队列中，等待M取出并运行 运行中(_Grunning)：表示M正在运行这个G，这时候M会拥有一个P 系统调用中(_Gsyscall)：表示M正在运行这个G发起的系统调用，这时候M并不拥有P 等待中(_Gwaiting)：表示G在等待某些条件完成，这时候G不在运行也不在运行队列中（可能在channel的等待队列中） 已终止(_Gdead)：表示G未被使用，可能已执行完毕（并在freelist中等待下次复用） 栈复制中(_Gcopystack)：表示G正在获取一个新的栈空间并把原来的内容复制过去（用于防止GC扫描）  M的状态 M并没有像G和P一样的状态标记，但可以认为一个M有以下的状态： - 自旋中(spinning)：M正在从运行队列获取G，这时候M会拥有一个P - 执行go代码中：M正在执行go代码，这时候M会拥有一个P - 执行原生代码中：M正在执行原生代码或者阻塞的syscall，这时M并不拥有P - 休眠中：M发现没有待运行的G时会进入休眠，并添加到空闲M链表中，这时M并不拥有P
自旋中这个状态非常重要，是否需要唤醒或者创建新的M取决于当前自旋中的M的数量。
P的状态  空闲中(_Pidle)：当M发现无待运行的G时会进入休眠，这时M拥有的P会变成空闲并加到空闲P链表中 运行中(_Prunning)：当M拥有了一个P后，这个P的状态就会变为运行中，M运行G会使用这个P中的资源。 系统调用中(_Psyscall)：当go调用原生代码，原生代码又反过来调用go代码时，使用的P会变成此状态 GC停止中(_Pgcstop)：当gc停止整个世界(STW)时，P会变为此状态。 已终止(_Pdead)：当P的数量在运行时改变，且数量减少时多余的P会变为此状态。  本地可运行队列G 在go中有多个运行队列可以保存待运行(_Grunnable)的G，他们分别是各个P中的本地运行队列和全局运行队列。</description>
    </item>
    
    <item>
      <title>什么是docker？</title>
      <link>https://realjf.io/posts/what-docker-is/</link>
      <pubDate>Tue, 19 Mar 2019 14:40:53 +0800</pubDate>
      
      <guid>https://realjf.io/posts/what-docker-is/</guid>
      <description>官方定义 Develop, Ship and Run Any Application, Anywhere Docker is a platform for developers and sysadmins to develop, ship, and run applications. Docker lets you quickly assemble applications from components and eliminates the friction that can come when shipping code. Docker lets you get your code tested and deployed into production as fast as possible.  Docker 是 PaaS 提供商 dotCloud 开源的一个基于 LXC 的高级容器引擎，源代码托管在 Github 上, 基于go语言并遵从Apache2.0协议开源。
 LXC linux container容器是一种内核虚拟化技术，可以提供轻量级的虚拟化，以便隔离进程和资源。与kvm之类最明显的区别在于启动快，资源占用小。</description>
    </item>
    
    <item>
      <title>Namespace 资源隔离</title>
      <link>https://realjf.io/posts/namespace/</link>
      <pubDate>Tue, 19 Mar 2019 14:38:54 +0800</pubDate>
      
      <guid>https://realjf.io/posts/namespace/</guid>
      <description>资源隔离 - linux有个chroot命令，可以实现资源隔离 主机隔离 网络隔离 进程间通信隔离 用户和用户组权限隔离 进程PID隔离  namespace 6项隔离    namespace 系统调用参数 隔离内容     UTS CLONE_NEWUTS 主机名与域名   IPC CLONE_NEWIPC 信号量、消息队列和共享内存   PID CLONE_NEWPID 进程编号   Network CLONE_NEWNET 网络设备、网络栈、端口等   Mount CLONE_NEWNS 挂载点（文件系统）   User CLONE_NEWUSER 用户和用户组     同一namespace下的进程可以感知彼此的变化，而对外界的进程一无所知。此处的namespace是指Linux内核3.8及以后版本。
 1. namespace api 4种操作方式 namespace的api包括clone()、setns()以及unshare()，还有/proc下的部分文件，
通过clone()在创建新进程的同时创建namespace 使用clone()来创建一个独立namespace的进程是常见方法，也是docker使用namespace最基本的方法：
int clone(int (*child_func)(void *), void *child_stack, int flags, void *arg);  查看/proc/[pid]/ns文件 用户就可以在/proc/[pid]/ns文件下看到指向不同namespace号的文件，形如[4034532445]者即为namespace号。</description>
    </item>
    
    <item>
      <title>Kubernetes集群下 Traefik安装和使用</title>
      <link>https://realjf.io/posts/k8s-plugins-traefik/</link>
      <pubDate>Tue, 19 Mar 2019 14:35:31 +0800</pubDate>
      
      <guid>https://realjf.io/posts/k8s-plugins-traefik/</guid>
      <description>前提：安装好kubernetes集群情况下
  run traefik and let it do the work for you!
 traefik官方地址：http://traefik.cn/
方法一：使用k8s安装 准备 # 创建目录 mkdir traefik cd traefik # 拉取traefik官方docker镜像 docker pull docker.io/traefik # docker hub地址：https://store.docker.com/images/traefik # 拉取traefik相关配置 git clone https://github.com/containous/traefik.git # 检查traefik配置 ll traefik/example/k8s/ -rw-r--r-- 1 root root 140 Sep 11 16:53 cheese-default-ingress.yaml -rw-r--r-- 1 root root 1805 Sep 11 16:53 cheese-deployments.yaml -rw-r--r-- 1 root root 519 Sep 11 16:53 cheese-ingress.yaml -rw-r--r-- 1 root root 509 Sep 11 16:53 cheese-services.</description>
    </item>
    
    <item>
      <title>kubernetes 核心原理之 网络模型</title>
      <link>https://realjf.io/posts/k8s-core-principle-network-model/</link>
      <pubDate>Tue, 19 Mar 2019 14:30:13 +0800</pubDate>
      
      <guid>https://realjf.io/posts/k8s-core-principle-network-model/</guid>
      <description>主要解决以下问题： - 容器与容器之间的直接通信 - pod与pod之间的通信 - pod到service之间的通信 - 集群外部与集群内部组件之间的通信
容器与容器之间的通信 同一个Pod内的容器共享同一个linux协议栈，可以用localhost地址访问彼此的端口 kubernetes利用docker的网桥与容器内的映射eth0设备进行通信
pod之间的通信 每个pod都拥有一个真实的全局ip地址 - 同一个node内的不同pod之间 可以直接采用对方的pod的ip地址通信（因为他们都在同一个docker0网桥上，属于同一地址段） - 不同node上的pod之间</description>
    </item>
    
    <item>
      <title>kubernetes 核心原理之 集群安全机制</title>
      <link>https://realjf.io/posts/k8s-core-principle-security/</link>
      <pubDate>Tue, 19 Mar 2019 14:29:44 +0800</pubDate>
      
      <guid>https://realjf.io/posts/k8s-core-principle-security/</guid>
      <description> 安全性考虑目标  保证容器与其所在的宿主机的隔离 限制容器给基础设施及其他容器带来消极影响的能力 最小权限原则 明确组件间边界的划分 划分普通用户和管理员的角色 在必要时允许将管理员权限赋给普通用户 允许拥有secret数据（Keys、Certs、Passwords）的应用在集群中运行  1. API Server认证管理（Authentication） 集群安全的关键就在于如何识别并认证客户端身份，以及随后访问权限的授权这两个关键问题
k8s提供3种级别的客户端身份认证方式： - 最严格的https证书认证：基于ca根证书签名的双向数字证书认证 - http token认证：通过一个token来识别合法用户
 http token用一个很长的特殊编码方式并且难以被模仿的字符串——token来表明客户端身份，每个token对应一个用户名，存储在api server能访问的一个文件中，当客户端发起api调用请求时，需要在http header里放入token，这样一来，api server就能识别合法用户和非法用户了。
  http base认证：通过用户名+密码的方式   http base是指把“用户名+冒号+密码”用base64算法进行编码后的字符串放在http request中的header authorization域里发送到服务端，服务端接受后进行解码，获取用户名及密码，然后进行用户身份鉴权过程
 2. API Server授权管理（Authorization） 通过授权策略来决定一个api调用是否合法。对合法用户进行授权并且随后在用户访问时进行鉴权，是权限与安全系统的重要一环。
目前支持的授权策略： - AlwaysDeny：表示拒绝所有的请求，一般用于测试 - AlwaysAllow：允许接受所有请求，如果集群不需要授权，则可以采用这个策略，这也是默认配置 - ABAC：基于属性的访问控制，表示使用用户配置的授权规则对用户请求进行匹配和控制。 - Webhook：通过调用外部rest服务对用户进行授权 - RBAC：基于角色的访问控制
ABAC授权模式 Webhook授权模式 RBAC授权模式详解 基于角色的访问控制： - 对集群中的资源和非资源权限均有完整的覆盖 - 整个RBAC完全由几个api对象完成，同其他api对象一样，可以用kubectl或api进行操作 - 可以在运行时进行调整，无需重新启动api server
 要使用RBAC授权模式，需要在api server的启动参数中加上 &amp;ndash;authorization-mode=RBAC
 3. Admission Control（准入控制） </description>
    </item>
    
    <item>
      <title>kubernetes 核心原理之 Kube-Proxy</title>
      <link>https://realjf.io/posts/k8s-core-principle-kube-proxy/</link>
      <pubDate>Tue, 19 Mar 2019 14:26:43 +0800</pubDate>
      
      <guid>https://realjf.io/posts/k8s-core-principle-kube-proxy/</guid>
      <description>service是一个抽象的概念，类似一个反向代理，将请求转发到后端的pod上。真正实现service作用的是kube-proxy服务进程。
在每个node上都会运行一个kube-proxy的服务进程，这个进程可以看做service的透明代理兼负载均衡器，其核心功能是将到某个service的访问请求转发到后端的多个pod实例上。
kube-proxy会在本地node上简历一个socketserver来负责接收请求，然后均匀发送到后端某个pod端口上，这个过程默认采用round robin负载均衡算法。
k8s也提供了通过修改service的service.spec.sessionAffinity参数的值来实现会话保持特性的定向转发，如果设置的值为“clientIp”，则将来自同一个clientip的请求都转发到同一个后端pod上。
 service的clusterIP与nodeport等概念是kube-proxy服务通过iptables的NAT转换实现的，kube-proxy在运行过程中动态创建与service相关的iptables规则
 访问service的请求，不论是用cluster ip+target port的方式，还是用节点机ip+node port的方式，都被节点机的iptables规则重定向到kube-proxy监听的service服务代理端口。
 kube-proxy的负载均衡器只支持round robin算法。同时在此基础上还支持session保持。
 kube-proxy内部创建了一个负载均衡器——loadbalancer，loadbalancer上保存了service到对应的后端endpoint列表的动态转发路由表，而具体的路由选择取决于round robin负载均衡算法及service的session会话保持（SessionAffinity）这两个特性
kube-porxy针对变化的service列表，处理流程  如果service没有设置集群ip（ClusterIP），则不做处理，否则，获取该service的所有端口定义列表（spec.ports域） 逐个读取服务端口定义列表中的端口信息，根据端口名称，service名称和namespace判断本地是否已经存在对应的服务代理对象，如不存在则新建，如存在且service端口被修改过，则先删除iptables中和srevice端口相关的规则，关闭服务代理对象，然后走新建流程。 更新负载均衡器组件中对应service的转发地址列表，对于新建的service，确定转发时的会话保持策略。 对于已经删除的service则进行清理   针对Endpoint的变化，kube-proxy会自动更新负载均衡器中对应service的转发地址列表。
 针对iptables所做的一些细节操作  KUBE-PORTALS-CONTAINER：从容器中通过service cluster ip和端口号访问service的请求。（容器） KUBE-PORTALS-HOST：从主机中通过service cluster ip和端口号访问service的请求（主机） KUBE-NODEPORT-CONTAINER：从容器中通过service的nodeport端口号访问service的请求。（容器） KUBE-NODEPORT-HOST：从主机中通过service的nodeport端口号访问service请求（主机）  此外，kube-proxy在iptables中为每个service创建由cluster ip+service端口号到kube-proxy所在主机ip+service代理服务所监听的端口的转发规则。
service类型为NodePort kube-proxy在iptables中除了添加上面提及的规则，还会为每个service创建由nodeport端口到kube-proxy所在主机ip+service代理服务所监听的端口的转发规则。</description>
    </item>
    
    <item>
      <title>kubernetes 核心原理之 Kubelet</title>
      <link>https://realjf.io/posts/k8s-core-principle-kubelet/</link>
      <pubDate>Tue, 19 Mar 2019 14:26:28 +0800</pubDate>
      
      <guid>https://realjf.io/posts/k8s-core-principle-kubelet/</guid>
      <description>在每个Node节点上都会启动一个kubelet服务进程，该进程负责处理master节点下发到本节点的任务，管理Pod和pod中的容器。
每个kubelet进程会在api server上注册节点自身信息，定期向master节点汇报节点资源的使用情况，并通过cAdvisor监控容器和节点资源。
节点管理 节点通过设置kubelet的启动参数“&amp;ndash;register-node”，来决定是否向api server注册自己，如果该参数为true，则会向api server注册自己。
其他参数包括： - &amp;ndash;api-servers：api server的位置 - &amp;ndash;kubeconfig：kubeconfig文件，用于访问api server的安全配置文件 - &amp;ndash;cloud-provider：云服务商地址，仅用于公有云环境
通过kubelet的启动参数“&amp;ndash;node-status-update-frequency”设置kubelet每隔多长时间想api server报告节点状态，默认是10s。
Pod管理 kubelet通过以下几种方式获取自身node上所要运行的pod清单： - 文件：同过启动参数“&amp;ndash;config”指定的配置文件目录下的文件（默认/etc/kubernetes/manifests/） - http断电：通过“&amp;ndash;manifest-url”参数设置 - api server：通过api server监听etcd目录，同步pod列表
kubelet去读监听到的信息，如果是创建和修改pod任务，则  为该pod创建一个数据目录 从api server读取该pod清单 为该pod挂载外部卷 下载pod用到的secret 检查已经运行在节点中的pod，如果该pod没有容器或pause容器（kubernetes/pause镜像创建的容器）没有启动，则先停止pod里所有容器的进程。如果在pod中有需要删除的容器，则删除这些容器。 用&amp;rdquo;kubernetes/pause&amp;rdquo;镜像为每个pod创建一个容器，该pause容器用于接管pod中所有其他容器的网络。每创建一个新的pod，kubelet都会先创建一个pause容器，然后创建其他容器。 为pod中的每个容器做如下处理： 为容器计算一个hash值，然后用容器的名字去查询对应docker容器的hash值。若找到容器，且两者的hash值不同，则停止docker中容器的进程，并停止与之关联的pause容器的进程，若两者相同，则不做任何处理。 如果容器被终止了，且容器没有指定的restartPolicy（重启策略），则不做任何处理。 调用docker client下载容器镜像，调用docker client运行容器。  容器健康检查 检查容器健康状态的两种探针 - LivenessProbe探针：判断容器是否健康，如果不健康，则删除Pod，根据其重启策略做相应处理。 - ReadinessProbe探针：判断容器是否完成启动，且准备接受请求。如果失败，pod的状态将被修改，Endpoint Controller将从Service的Endpoint中删除包含该容器所在pod的ip地址的endpoint条目。
LivenessProbe实现方式  ExecAction：在容器内部执行一个命令，如果该命令的退出状态码为0，则表明容器健康 TCPSocketAction：通过容器的ip地址和端口号执行TCP检查，如果端口能被访问，则表明容器健康 HTTPGetAction：通过容器的ip地址和端口号即路径调用http get方法，如果响应的状态码大于等于200且小于400，则认为容器状态健康  LivenessProbe探针包含在pod定义的spec.containers.{某个容器}中
# 容器命令检查 livenessProbe: exec: command: - cat - /tmp/health initialDelaySeconds: 15 timeoutSeconds: 1  # http检查 livenessProbe: httpGet: path: /healthz port: 8080 initialDelaySeconds: 15 timeoutSeconds: 1  cAdvisor资源监控 监控级别包括：容器、pod、service和整个集群</description>
    </item>
    
    <item>
      <title>kubernetes 核心原理之 Sheduler</title>
      <link>https://realjf.io/posts/k8s-core-principle-sheduler/</link>
      <pubDate>Tue, 19 Mar 2019 14:26:15 +0800</pubDate>
      
      <guid>https://realjf.io/posts/k8s-core-principle-sheduler/</guid>
      <description>作用是将待调度的pod按照特定的调度算法和调度策略绑定到集群中的某个合适的Node上，并将绑定信息写入etcd中。
目标节点上的kubelet通过api server监听到schduler产生的pod绑定事件，然后获取对应的pod清单，下载image镜像，并启动容器。
Scheduler默认调度流程分为以下两步  预调度过程，即遍历所有目标node，筛选出符合要求的候选节点 确定最优节点，在上一步基础上，采用优选策略计算出每个候选节点的积分，积分高者胜出。  Scheduler调度流程是通过插件方式加载的“调度算法提供者”（AlgorithmProvider）具体实现的。一个AlgorithmProvider其实是一组预选策略与一组优先选择策略的结构体。
Scheduler中可选的预选策略  NoDiskConflict PodFitsResources PodSelectorMatches PodFitsHost CheckNodeLabelPresence CheckServiceAffinity PodFitsPorts  Scheduler优选策略  LeastRequestedPriority（资源消耗最小） CalculateNodeLabelPriority BalancedResourceAllocation（各项资源使用率最均衡的节点）  每个节点通过优选策略算出一个得分，最终选出分值最大的节点作为优选的结果。</description>
    </item>
    
    <item>
      <title>kubernetes 核心原理之 Controller Manager</title>
      <link>https://realjf.io/posts/k8s-core-principle-controller-manager/</link>
      <pubDate>Tue, 19 Mar 2019 14:26:00 +0800</pubDate>
      
      <guid>https://realjf.io/posts/k8s-core-principle-controller-manager/</guid>
      <description>controller manager作为集群内部的管理控制中心，负责集群内的Node、pod副本、服务端（Endpoint）、命名空间（Namespace）、服务账号（ServiceAccount）、资源定额（ResourceQuota）等的管理。
controller manager组件  replication controller node controller resourceQuota controller namespace controller serviceAccount controller token controller service controller endpoint controller  1. Replication Controller（副本控制器） 核心作用是确保在任何时候集群中一个RC所关联的pod副本数量保持预设值。 &amp;gt; 只有当pod的重启策略是always时（RestartPolicy=Always），Replication Controller才会管理该Pod的操作（创建、销毁、重启等）。
RC中的Pod模板就像一个模具，一旦pod被创建完毕，它们之间就没有关系了。
此外，可以通过修改Pod的标签来实现脱离RC的管控。该方法可以用于将pod从集群中迁移、数据修复等调试。
Replication Controller职责  确保当前集群中有且仅有N个pod实例，N是RC中定义的pod副本数量 通过调整RC的spec.replicas属性值来实现系统扩容或者缩容 通过改变RC中的pod模板（主要是镜像版本）来实现系统的滚动升级  Replication Controller典型使用场景  重新调度（Rescheduling） 弹性伸缩（Scaling） 滚动更新（Rolling Updates）  2. Node Controller Node Controller通过API Server实时获取Node的相关信息：节点健康状况、节点资源、节点名称、节点地址信息、操作系统版本、Docker版本、kubelet版本等。
node controller节点信息更新机制 比较节点信息和node controller的nodeStatusMap中保存的节点信息 - 如果没有收到kubelet发送的节点信息、第一次收到节点kubelet发送的节点信息，或处理过程中节点状态变成非健康状态，则在nodeStatusMap中保存改节点的状态信息，并用node controller所在节点的系统时间作为探测时间和节点状态变化时间。 - 如果指定时间内收到新的节点信息，且节点状态发生变化，则在nodeStatusMap保存改节点的状态信息，并用node controller所在节点的系统时间作为探测时间，用上次节点信息中的节点状态变化时间作为该节点的状态变化时间 - 如果某一段时间内没有收到该节点状态信息，则设置节点状态为未知，并通过api server保存节点状态
3. ResourceQuota Controller（资源配额管理） 资源配额管理确保了指定的资源对象在任何时候都不会超量占用系统物理资源，避免由于某些业务进程的设计或实现的缺陷导致整个系统运行紊乱甚至意外宕机</description>
    </item>
    
    <item>
      <title>kubernetes 核心原理之 Apiserver</title>
      <link>https://realjf.io/posts/k8s-core-principle-apiserver/</link>
      <pubDate>Tue, 19 Mar 2019 14:25:43 +0800</pubDate>
      
      <guid>https://realjf.io/posts/k8s-core-principle-apiserver/</guid>
      <description>API Server的核心功能提供了kubernetes各类资源对象（如pod、rc、service等）的增删改查以及watch等http rest接口，是集群内各个功能模块之间数据交互和通信的中心枢纽，是整个系统的数据总线和数据中心。
 是集群管理的api入口 是资源配额控制的入口 提供了完备的集群安全机制  </description>
    </item>
    
    <item>
      <title>kubernetes 基本概念和术语</title>
      <link>https://realjf.io/posts/k8s-basic-concepts-and-terminology/</link>
      <pubDate>Tue, 19 Mar 2019 14:21:00 +0800</pubDate>
      
      <guid>https://realjf.io/posts/k8s-basic-concepts-and-terminology/</guid>
      <description>kubernetes中大部分概念，如node、pod、replication、controller、service等都可以看作是一种“资源对象”，几乎所有的资源对象都可以通过kubernetes提供的kubectl工具执行增、删、改、查等操作并保存在etcd中持久化存储。
k8s里所有资源对象都可以采用yaml或者json格式的文件来定义或描述
 1. Master（主节点、集群控制节点）  每个kubernets集群里需要有一个master节点来负责整个集群管理和控制 所有控制命令都发给它 占据一个独立的服务器 如果宕机或不可用，整个集群内容器应用的管理都将失效  master节点运行一组以下关键进程  kubernetes api server(kube-apiserver)：提供http rest接口，是k8s所有资源增删改查等操作的唯一入口，也是集群控制入口进程 kubernetes controller manager(kube-controller-manager)：k8s所有资源对象的自动化控制中心 kubernetes scheduler(kube-scheduler)：负责资源调度（pod调度）的进程 etcd服务：保存k8s所有资源对象的数据  相关命令  kubectl get nodes：查看集群有多少个node kubectl describe node ：查看某个node详细信息  2. Node（较早版本也叫minion）  节点既可以是物理机，也可以是私有云或者公有云中的一个虚拟机，通常在一个节点上运行几百个pod kubernetes集群中的工作负载节点，当某个node宕机，其上的工作负载会被master自动转移到其他节点上  每个node节点运行一组以下关键进程  kubelet：负责pod对应的容器的创建、启停等，同时与master节点密切协作，实现集群管理的基本功能 kube-proxy：实现kubernetes service的通信与负载均衡机制 docker engine：docker引擎，负责本机的容器创建和管理工作  3. Pod 是k8s最重要也是最基本概念 - 每个Pod都有一个特殊的被称为“根容器”的Pause容器，Pause容器对应的镜像属于k8s平台的一部分（gcr.io/google_containers/pause-amd64） - pod对象将每个服务进程包装到相应的pod中，使其成为pod中运行的一个容器 - 根容器不易死亡 - pod里的多个业务容器共享pause容器的ip，共享pause容器挂接的volume（解决Pod直接拿文件共享问题） - k8s为每个pod都分配唯一的ip地址，称之为pod ip，一个Pod里的多个容器共享pod ip地址 - 集群内任意两个pod之间的tcp/ip可以直接通信，通常采用虚拟二层网络技术实现，如：flannel、open vSwitch等。
pod的两种类型  普通的pod（存放在k8s的etcd中） 静态pod（存放在某个具体的node上的一个具体文件中，且只在此Node上启动运行）   默认情况下：当pod里的某个容器停止时，k8s会自动检测到这个问题并重新启动这个pod（重启pod里的所有容器），如果pod所在node宕机，则会将这个Node上的所有pod重新调度到其他节点上。</description>
    </item>
    
    <item>
      <title>Kubernetes集群搭建三 之 docker镜像配置</title>
      <link>https://realjf.io/posts/k8s-cluster-set-up-3/</link>
      <pubDate>Tue, 19 Mar 2019 14:13:57 +0800</pubDate>
      
      <guid>https://realjf.io/posts/k8s-cluster-set-up-3/</guid>
      <description> 1. 使用docker提供的registry镜像创建一个私有镜像仓库 具体可以参考 https://docs.docker.com/registry/deploying
运行以下命令，启动一个本地镜像仓库 docker 1.6以上版本可以直接运行以下命令
docker run -d -p 5000:5000 --restart=always --name registry registry:2  停止本地仓库
docker container stop registry &amp;amp;&amp;amp; docker container rm -v registry  镜像仓库操作
docker pull ubuntu docker image tag ubuntu localhost:5000/myfirstimage docker push localhost:5000/myfirstimage docker pull localhost:5000/myfirstimage  2. kubelet配置 k8s中docker以pod启动，在kubelet创建pod时，还通过启动一个名为gcr.io/google_containers/pause的镜像来实现pod的概念。
需要从gcr.io中将该镜像下载，导出文件，再push到私有docker registry中。之后，可以给每台node的kubelet服务加上启动参数&amp;ndash;pod-infra-container-image，指定为私有仓库中pause镜像的地址。
--pod-infra-container-image=gcr.io/google_containers/pause-amd64:3.0  如果镜像无法下载，可以从docker hub上进行下载：
docker pull kubeguide/pause-amd64:3.0  然后在kubelet启动参数加上该配置，重启kubelet服务即可
systemctl restart kubelet  </description>
    </item>
    
    <item>
      <title>Kubernetes集群搭建二 之 k8s集群</title>
      <link>https://realjf.io/posts/k8s-cluster-set-up-2/</link>
      <pubDate>Tue, 19 Mar 2019 14:13:53 +0800</pubDate>
      
      <guid>https://realjf.io/posts/k8s-cluster-set-up-2/</guid>
      <description>方式1：基于CA签名的双向数字证书认证方式 过程如下： - 为kube-apiserver生成一个数字证书，并用CA证书进行签名 - 为kube-apiserver进程配置证书相关的启动参数，包括CA证书（用于验证客户端证书的签名真伪）、自己的经过CA签名后的证书及私钥 - 为每个访问K8S API server的客户端进程生成自己的数字证书，也都用CA证书进行签名，在相关程序的启动参数里增加CA证书、自己的证书等相关参数
1). 设置kube-apiserver的CA证书相关的文件和启动参数 使用openssl工具在master服务器上创建CA证书和私钥相关的文件：
openssl genrsa -out ca.key 2048 openssl req -x509 -new -nodes -key ca.key -subj &amp;quot;/CN=k8s-master&amp;quot; -days 5000 -out ca.crt openssl genrsa -out server.key 2048  注：生成ca.crt时，-subj参数中“/CN”的值为Master主机名
 509是一种通用的证书格式
 准备master_ssl.cnf文件，用于x509 v3版本的证书，示例如下：
[ req ] req_extensions = v3_req distinguished_name = req_distinguished_name [ req_distinguished_name ] [ v3_req ] basicConstraints = CA:FALSE keyUsage = nonRepudiation, digitalSignature, keyEncipherment subjectAltName = @alt_names [alt_names] DNS.</description>
    </item>
    
    <item>
      <title>Kubernetes集群搭建一 之 etcd集群</title>
      <link>https://realjf.io/posts/k8s-cluster-set-up-1/</link>
      <pubDate>Tue, 19 Mar 2019 14:13:46 +0800</pubDate>
      
      <guid>https://realjf.io/posts/k8s-cluster-set-up-1/</guid>
      <description>系统要求    软硬件 最低配置 推荐配置     cpu和内存 master:至少2core和4GB内存 Node：至少4core和16GB Master:4core和16GB Node: 应根据需要运行的容器数量进行配置   linux操作系统 基于x86_64架构的各种linux发行版本 Red Hat Linux 7 CentOS 7   Docker 1.9版本以上 1.12版本   etcd 2.0版本及以上 3.0版本    本次实验选用的是centos7 1804版本
 需要注意，kubernetes的master和node节点之间会有大量的网络通信，安全的做法是在防火墙上配置各组件需要相互通信的端口号。在一个安全的内网环境中，可以关闭防火墙服务
#关闭防火墙 systemctl disable firewalld systemctl stop firewalld # 禁用SELinux setenforce 0 # 也可以修改/etc/sysconfig/selinux，将SELINUX=enforcing修改成SELINUX=disabled   这里将搭建一个master节点和一个node节点的k8s集群  由于 raft 算法的特性，集群的节点数必须是奇数
    - ip etcd节点名称     master节点 192.</description>
    </item>
    
    <item>
      <title>How to Set Up Blog Using Hugo</title>
      <link>https://realjf.io/posts/how-to-set-up-blog-using-hugo/</link>
      <pubDate>Tue, 19 Mar 2019 09:43:09 +0800</pubDate>
      
      <guid>https://realjf.io/posts/how-to-set-up-blog-using-hugo/</guid>
      <description>github pages有两种方式：  一种是{USERNAME}.github.io/ 另一种是{USERNAME}.github.io/{PROJECT}  我们这里使用第二种方法创建
前期准备  有一个github账号  创建一个公开的repo 例如：blog
开通github pages 找到新创建的repo中的settings，往下找到github pages， 如果首次开通，则需要授权一下，授权后，github pages下的source可以选择对应的发布分支。默认为master分支。
注意 如果一切正常，github pages选项下有个蓝色提示，显示的是您的博客地址，可以先访问看看是否正常。我这里是：https://realjf.github.io/blog/
配置好后，开始使用hugo构建博客 首先，clone下刚才创建的repo
git clone git@github.com:{USERNAME}/blog  安装hugo，确保repo目录下可以使用hugo命令 请参考官网https://gohugo.io/
# 检查安装是否成功 hugo version  利用hugo构建博客目录结构 cd blog &amp;amp;&amp;amp; hugo new site . --force  这里使用了&amp;ndash;force是因为当前目录已存在，只是需要初始化而已
添加自己需要的主题 cd blog git submodule add https://github.com/realjf/hugo-theme-m10c.git themes/m10c  上述的m10c可以换成你想要的主题名字即可
更多的主题请参考：https://themes.gohugo.io/
# 修改根目录下的 .toml文件 theme = &amp;quot;{THEME}&amp;quot; baseUrl = &amp;quot;https://realjf.github.io/blog/&amp;quot;  {THEME}请修改为你的主题名即可
本地测试博客 hugo server -t {THEME}  到这里，基本的博客搭建完成，先保存到github git add -A &amp;amp;&amp;amp; git commit -m &amp;quot;Initializing&amp;quot; git push origin master  本地测试成功后，我们利用gh-pages分支作为新的发布分支 gh-pages分支保存的是hugo生成的html静态文件</description>
    </item>
    
  </channel>
</rss>